{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "339cc92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Kiran\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kiran\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kiran\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Kiran\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Kiran\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Kiran\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Kiran\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Bidirectional, LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score, cohen_kappa_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import emoji\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize stop words and lemmatizer\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0b3303f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Varials\n",
    "DATASET_DIR = os.path.join('..', 'Dataset')\n",
    "MODEL_DIR = os.path.join('..', 'Model')\n",
    "IMAGE_DIR = os.path.join('..', 'Image')\n",
    "\n",
    "DATA_PATH = os.path.join(DATASET_DIR, 'Suicide_Detection.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a03c3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def read_csv(csv_file_path):\n",
    "    # read csv\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    # replace class column values\n",
    "    df['class'].replace({'suicide':1 , 'non-suicide':0} , inplace = True)\n",
    "    \n",
    "    # show head\n",
    "    display(df.head())\n",
    "    print('_'*50)\n",
    "    \n",
    "    # show info\n",
    "    display(df.info())\n",
    "    print('_'*50)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_model(model, file_path):\n",
    "    \"\"\"\n",
    "    Saves the Keras Model to a .pkl file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(tokenizer, file)\n",
    "        \n",
    "    print(f\"Model saved to {file_path}\")\n",
    "    \n",
    "def load_model(file_path):\n",
    "    \"\"\"\n",
    "    Loads the Keras Tokenizer from a .pkl file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "        \n",
    "    print(f\"Model loaded from {file_path}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "'''\n",
    "Use example:\n",
    "# Load the tokenizer when needed\n",
    "loaded_tokenizer = load_model(\"tokenizer.pkl\")\n",
    "# Use the loaded tokenizer for new data\n",
    "new_texts = [\"I enjoy coding\"]\n",
    "new_sequences = loaded_tokenizer.texts_to_sequences(new_texts)\n",
    "new_x_data = pad_sequences(new_sequences, maxlen=max_len)\n",
    "print(\"Transformed new data:\", new_x_data)\n",
    "'''\n",
    "\n",
    "def preprocess_tokenization_padding(df):\n",
    "    # split text and label\n",
    "    texts = data['text'].values\n",
    "    labels = data['class'].values\n",
    "    \n",
    "    # Tokenization and padding\n",
    "    max_words = 20000  # Adjust based on vocabulary size\n",
    "    max_len = 200  # Adjust based on average post length\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    \n",
    "    # save model to pkl\n",
    "    model_file_path = os.path.join(MODEL_DIR, 'tokenizer.csv')\n",
    "    save_to_pkl(tokenizer, model_file_path)\n",
    "    \n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    x_data = pad_sequences(sequences, maxlen=max_len)\n",
    "    y_data = np.array(labels)\n",
    "    \n",
    "    return x_data, y_data \n",
    "\n",
    "def split_data(x_data, y_data, test_size=0.2, random_state=42):\n",
    "    '''\n",
    "    Split the data into train, val, and test dataset\n",
    "    '''\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, \n",
    "                                                      test_size = test_size, \n",
    "                                                      random_state = random_state, \n",
    "                                                      stratify = y_data)\n",
    "    \n",
    "    x_test, _, y_test, _ = train_test_split(x_train, y_train, \n",
    "                                            test_size = test_size, \n",
    "                                            random_state = random_state, \n",
    "                                            stratify = y_train)\n",
    "    \n",
    "    return (x_train, y_train), (x_val, y_val), (x_test, y_test)\n",
    "\n",
    "def compile_model(model, optimizer='adam', loss='binary_crossentropy', metrics='accuracy'):\n",
    "    '''\n",
    "    To compile the model\n",
    "    '''\n",
    "    # Compile the model\n",
    "    model.compile(optimizer = optimizer, \n",
    "                  loss = loss, \n",
    "                  metrics = [metrics])\n",
    "\n",
    "    # Model summary\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def plot_model(history):\n",
    "    NAME_PREFIX = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    \n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(1, len(acc)+1)\n",
    "\n",
    "    plt.plot(epochs, acc, label='training accuracy')\n",
    "    plt.plot(epochs, val_acc, label='validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('Training validation Accuracy')\n",
    "    plt.savefig(os.path.join(IMAGE_DIR, f'{NAME_PREFIX}_accuracy_and_val_accuracy.png'), dpi=200)\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(epochs, loss, label='training loss')\n",
    "    plt.plot(epochs, val_loss, label='validation loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title('Training validation Loss')\n",
    "    plt.savefig(os.path.join(IMAGE_DIR, f'{NAME_PREFIX}_loss_and_val_loss.png'), dpi=200)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'Image saved to {IMAGE_DIR} - [{NAME_PREFIX}_accuracy_and_val_accuracy.png, {NAME_PREFIX}_loss_and_val_loss.png]')\n",
    "    \n",
    "def train_model(model, batch_size=32, epochs=10, x_train, y_train, x_val, y_val):\n",
    "    '''\n",
    "    Train the model\n",
    "    '''\n",
    "    history = model.fit(x_train, y_train, \n",
    "                        epochs = epochs, \n",
    "                        batch_size = batch_size, \n",
    "                        validation_data = (x_val, y_val), \n",
    "                        verbose=1)\n",
    "    plot_model(history)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def evaluate_model():\n",
    "    ...\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e2c74d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "126c3e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20241116220121'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad262db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
