

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score, cohen_kappa_score
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
import xgboost as xgb
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, LSTM, Bidirectional, GRU, SimpleRNN
from transformers import BertTokenizer, TFBertModel
from tensorflow.keras.optimizers import Adam
from sklearn.feature_extraction.text import TfidfVectorizer

# Load Data
data = pd.read_csv('suicidal_ideation_data.csv')
texts = data['text'].values
labels = data['label'].values

# Split Data
x_train, x_val, y_train, y_val = train_test_split(texts, labels, test_size=0.2, random_state=42)

# TF-IDF Vectorization for traditional ML models
vectorizer = TfidfVectorizer(max_features=5000)
x_train_tfidf = vectorizer.fit_transform(x_train).toarray()
x_val_tfidf = vectorizer.transform(x_val).toarray()

# Initialize evaluation metrics dictionary
evaluation_metrics = {}

# Function to evaluate models
def evaluate_model(model_name, y_true, y_pred):
    report = classification_report(y_true, y_pred, output_dict=True)
    evaluation_metrics[model_name] = {
        'accuracy': accuracy_score(y_true, y_pred),
        'precision': precision_score(y_true, y_pred),
        'recall': recall_score(y_true, y_pred),
        'f1_score': f1_score(y_true, y_pred),
        'kappa': cohen_kappa_score(y_true, y_pred),
        'classification_report': report
    }
    print(f"{model_name} - Classification Report:\n", classification_report(y_true, y_pred))

### Traditional ML Models ###

# Logistic Regression
log_reg = LogisticRegression()
log_reg.fit(x_train_tfidf, y_train)
y_pred = log_reg.predict(x_val_tfidf)
evaluate_model("Logistic Regression", y_val, y_pred)

# Naive Bayes
nb = MultinomialNB()
nb.fit(x_train_tfidf, y_train)
y_pred = nb.predict(x_val_tfidf)
evaluate_model("Naive Bayes", y_val, y_pred)

# Decision Tree
dt = DecisionTreeClassifier()
dt.fit(x_train_tfidf, y_train)
y_pred = dt.predict(x_val_tfidf)
evaluate_model("Decision Tree", y_val, y_pred)

# Random Forest
rf = RandomForestClassifier()
rf.fit(x_train_tfidf, y_train)
y_pred = rf.predict(x_val_tfidf)
evaluate_model("Random Forest", y_val, y_pred)

# Support Vector Machine
svm = SVC()
svm.fit(x_train_tfidf, y_train)
y_pred = svm.predict(x_val_tfidf)
evaluate_model("SVM", y_val, y_pred)

# XGBoost
xgb_model = xgb.XGBClassifier()
xgb_model.fit(x_train_tfidf, y_train)
y_pred = xgb_model.predict(x_val_tfidf)
evaluate_model("XGBoost", y_val, y_pred)

### Deep Learning Models ###

# Prepare text data for deep learning
tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=5000)
tokenizer.fit_on_texts(x_train)
x_train_seq = tokenizer.texts_to_sequences(x_train)
x_val_seq = tokenizer.texts_to_sequences(x_val)
x_train_pad = tf.keras.preprocessing.sequence.pad_sequences(x_train_seq, maxlen=100)
x_val_pad = tf.keras.preprocessing.sequence.pad_sequences(x_val_seq, maxlen=100)

def build_cnn_model():
    model = Sequential([
        Embedding(input_dim=5000, output_dim=128, input_length=100),
        Conv1D(64, 5, activation='relu'),
        GlobalMaxPooling1D(),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

def build_rnn_model():
    model = Sequential([
        Embedding(input_dim=5000, output_dim=128, input_length=100),
        SimpleRNN(64),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

def build_lstm_model():
    model = Sequential([
        Embedding(input_dim=5000, output_dim=128, input_length=100),
        LSTM(64),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

def build_bilstm_model():
    model = Sequential([
        Embedding(input_dim=5000, output_dim=128, input_length=100),
        Bidirectional(LSTM(64)),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

def build_gru_model():
    model = Sequential([
        Embedding(input_dim=5000, output_dim=128, input_length=100),
        GRU(64),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# CNN
cnn_model = build_cnn_model()
cnn_model.fit(x_train_pad, y_train, epochs=3, batch_size=32, validation_data=(x_val_pad, y_val))
y_pred = (cnn_model.predict(x_val_pad) > 0.5).astype("int32")
evaluate_model("CNN", y_val, y_pred)

# RNN
rnn_model = build_rnn_model()
rnn_model.fit(x_train_pad, y_train, epochs=3, batch_size=32, validation_data=(x_val_pad, y_val))
y_pred = (rnn_model.predict(x_val_pad) > 0.5).astype("int32")
evaluate_model("RNN", y_val, y_pred)

# LSTM
lstm_model = build_lstm_model()
lstm_model.fit(x_train_pad, y_train, epochs=3, batch_size=32, validation_data=(x_val_pad, y_val))
y_pred = (lstm_model.predict(x_val_pad) > 0.5).astype("int32")
evaluate_model("LSTM", y_val, y_pred)

# BiLSTM
bilstm_model = build_bilstm_model()
bilstm_model.fit(x_train_pad, y_train, epochs=3, batch_size=32, validation_data=(x_val_pad, y_val))
y_pred = (bilstm_model.predict(x_val_pad) > 0.5).astype("int32")
evaluate_model("BiLSTM", y_val, y_pred)

# GRU
gru_model = build_gru_model()
gru_model.fit(x_train_pad, y_train, epochs=3, batch_size=32, validation_data=(x_val_pad, y_val))
y_pred = (gru_model.predict(x_val_pad) > 0.5).astype("int32")
evaluate_model("GRU", y_val, y_pred)

### BERT Model ###

# BERT Tokenizer and Model Preparation
bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = TFBertModel.from_pretrained('bert-base-uncased')

def bert_encode(texts, tokenizer, max_len=100):
    tokens = tokenizer(texts, padding='max_length', max_length=max_len, truncation=True, return_tensors='tf')
    return tokens['input_ids'], tokens['attention_mask']

x_train_input_ids, x_train_attention = bert_encode(x_train, bert_tokenizer)
x_val_input_ids, x_val_attention = bert_encode(x_val, bert_tokenizer)

def build_bert_model():
    input_ids = tf.keras.layers.Input(shape=(100,), dtype=tf.int32, name="input_ids")
    attention_mask = tf.keras.layers.Input(shape=(100,), dtype=tf.int32, name="attention_mask")
    bert_output = bert_model([input_ids, attention_mask])[1]
    output = Dense(1, activation='sigmoid')(bert_output)
    model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)
    model.compile(Adam(lr=3e-5), loss='binary_crossentropy', metrics=['accuracy'])
    return model

bert_model = build_bert_model()
bert_model.fit([x_train_input_ids, x_train_attention], y_train, validation_data=([x_val_input_ids, x_val_attention], y_val), epochs=3, batch_size=16)
y_pred = (bert_model.predict([x_val_input_ids, x_val_attention]) > 0.5).astype("int32")
evaluate_model("BERT", y_val, y_pred)

# Display all evaluation metrics
for model_name, metrics in evaluation_metrics.items():
    print(f"--- {model_name} ---")
    for metric, value in metrics.items():
        if metric != 'classification_report':
            print(f"{metric}: {value:.4f}")



streamlit run app.py