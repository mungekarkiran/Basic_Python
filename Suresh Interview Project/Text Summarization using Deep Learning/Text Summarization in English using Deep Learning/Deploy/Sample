The first paper on zero-shot learning in natural language processing appeared in 2008 at the AAAI’08, but the name given to the learning paradigm there was dataless classification. The first paper on zero-shot learning in computer vision appeared at the same conference, under the name zero-data learning. The term zero-shot learning itself first appeared in the literature in a 2009 paper from Palatucci, Hinton, Pomerleau, and Mitchell at NIPS’09. This terminology was repeated later in another computer vision paper and the term zero-shot learning caught on, as a take-off on one-shot learning that was introduced in computer vision years earlier.
In computer vision, zero-shot learning models learned parameters for seen classes along with their class representations and rely on representational similarity among class labels so that, during inference, instances can be classified into new classes.
In natural language processing, the key technical direction developed builds on the ability to "understand the labels"—represent the labels in the same semantic space as that of the documents to be classified. This supports the classification of a single example without observing any annotated data, the purest form of zero-shot classification. The original paper made use of the Explicit Semantic Analysis (ESA) representation but later papers made use of other representations, including dense representations. This approach was also extended to multilingual domains, fine entity typing and other problems. Moreover, beyond relying solely on representations, the computational approach has been extended to depend on transfer from other tasks, such as textual entailment and question answering.
The original paper also points out that, beyond the ability to classify a single example, when a collection of examples is given, with the assumption that they come from the same distribution, it is possible to bootstrap the performance in a semi-supervised like manner (or transductive learning).
Unlike standard generalization in machine learning, where classifiers are expected to correctly classify new samples to classes they have already observed during training, in ZSL, no samples from the classes have been given during training the classifier. It can therefore be viewed as an extreme case of domain adaptation.


प्राकृतिक भाषा प्रसंस्करण में शून्य-शॉट सीखने पर पहला पेपर 2008 में AAAI'08 में प्रकाशित हुआ था, लेकिन वहां सीखने के प्रतिमान को जो नाम दिया गया था वह डेटालेस वर्गीकरण था। कंप्यूटर विज़न में ज़ीरो-शॉट लर्निंग पर पहला पेपर ज़ीरो-डेटा लर्निंग नाम से उसी सम्मेलन में प्रकाशित हुआ। ज़ीरो-शॉट लर्निंग शब्द पहली बार साहित्य में 2009 में NIPS'09 में पलाटुकी, हिंटन, पोमेरलेउ और मिशेल के पेपर में सामने आया था। इस शब्दावली को बाद में एक अन्य कंप्यूटर विज़न पेपर में दोहराया गया और ज़ीरो-शॉट लर्निंग शब्द को एक-शॉट लर्निंग के रूप में अपनाया गया, जिसे वर्षों पहले कंप्यूटर विज़न में पेश किया गया था।
कंप्यूटर विज़न में, शून्य-शॉट लर्निंग मॉडल ने देखी गई कक्षाओं के लिए उनके वर्ग प्रतिनिधित्व के साथ पैरामीटर सीखे और क्लास लेबल के बीच प्रतिनिधित्वात्मक समानता पर भरोसा किया, ताकि अनुमान के दौरान, उदाहरणों को नए वर्गों में वर्गीकृत किया जा सके।
प्राकृतिक भाषा प्रसंस्करण में, विकसित की गई मुख्य तकनीकी दिशा "लेबल को समझने" की क्षमता पर आधारित है - वर्गीकृत किए जाने वाले दस्तावेजों के समान अर्थ स्थान में लेबल का प्रतिनिधित्व करना। यह किसी भी एनोटेटेड डेटा को देखे बिना एकल उदाहरण के वर्गीकरण का समर्थन करता है, जो शून्य-शॉट वर्गीकरण का सबसे शुद्ध रूप है। मूल पेपर में एक्सप्लिसिट सिमेंटिक एनालिसिस (ईएसए) प्रतिनिधित्व का उपयोग किया गया था लेकिन बाद के पेपरों में सघन प्रतिनिधित्व सहित अन्य अभ्यावेदन का उपयोग किया गया। इस दृष्टिकोण को बहुभाषी डोमेन, बढ़िया इकाई टाइपिंग और अन्य समस्याओं तक भी बढ़ाया गया था। इसके अलावा, केवल अभ्यावेदन पर भरोसा करने से परे, कम्प्यूटेशनल दृष्टिकोण को अन्य कार्यों, जैसे पाठ्य सामग्री और प्रश्न उत्तर से स्थानांतरण पर निर्भर करने के लिए बढ़ाया गया है।
मूल पेपर यह भी बताता है कि, एकल उदाहरण को वर्गीकृत करने की क्षमता से परे, जब उदाहरणों का एक संग्रह दिया जाता है, इस धारणा के साथ कि वे एक ही वितरण से आते हैं, तो अर्ध-पर्यवेक्षित तरीके से प्रदर्शन को बूटस्ट्रैप करना संभव है (या ट्रांसडक्टिव लर्निंग)।
मशीन लर्निंग में मानक सामान्यीकरण के विपरीत, जहां क्लासिफायर से अपेक्षा की जाती है कि वे प्रशिक्षण के दौरान पहले से ही देखी गई कक्षाओं में नए नमूनों को सही ढंग से वर्गीकृत करें, ZSL में, क्लासिफायर के प्रशिक्षण के दौरान कक्षाओं से कोई नमूना नहीं दिया गया है। इसलिए इसे डोमेन अनुकूलन के चरम मामले के रूप में देखा जा सकता है।