{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Personality System\n",
    "\n",
    "Five Personality Traits (OCEAN)\n",
    "\n",
    "- Openness to experience (inventive/curious vs. consistent/cautious)\n",
    "- Conscientiousness (efficient/organized vs. easy-going/careless)\n",
    "- Extroversion (outgoing/energetic vs. solitary/reserved)\n",
    "- Agreeableness (friendly/compassionate vs. challenging/detached)\n",
    "- Neuroticism (sensitive/nervous vs. secure/confident)\n",
    "\n",
    "Resources: \n",
    "- [wikipedia](https://en.wikipedia.org/wiki/Big_Five_personality_traits)\n",
    "- [ipip.ori.org](https://ipip.ori.org/newBigFive5broadKey.htm)\n",
    "- [How Accurately Can You Describe Yourself?](https://ipip.ori.org/new_ipip-50-item-scale.htm)\n",
    "- [Dataset](https://www.kaggle.com/datasets/tunguz/big-five-personality-test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personality System\n",
    "\n",
    "#### read Codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lib's\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import random\n",
    "\n",
    "# for ploting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# clustering lib and Visualize the elbow\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "# For ease of calculation, scale all the values between 0-1 \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(r'dataset\\\\data-final.csv', delimiter='\\t')\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dataframe_list = [pd.read_csv(df_path) for df_path in glob.glob(r'dataset\\data_final\\*.csv')]\n",
    "# print(dataframe_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_csv(df_path) for df_path in glob.glob(r'dataset\\data_final\\*.csv')])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.columns\n",
    "print(columns)\n",
    "\n",
    "for c in columns:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = df[df.columns[0:50]]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(X.describe(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values\n",
    "\n",
    "# def plot_nas(df: pd.DataFrame):\n",
    "#     if df.isnull().sum().sum() != 0:\n",
    "#         na_df = (df.isnull().sum() / len(df)) * 100      \n",
    "#         na_df = na_df.drop(na_df[na_df == 0].index).sort_values(ascending=False)\n",
    "#         missing_data = pd.DataFrame({'Missing Ratio %' :na_df})\n",
    "#         missing_data.plot(kind = \"barh\")\n",
    "#         plt.show()\n",
    "#     else:\n",
    "#         print('No NAs found')\n",
    "# plot_nas(df)\n",
    "\n",
    "# https://dev.to/tomoyukiaota/visualizing-the-patterns-of-missing-value-occurrence-with-python-46dj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No nan value : for safety purposes fill nan with 0\n",
    "X = X.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find number of clusters\n",
    "columns = list(X.columns)\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "data = scaler.fit_transform(X)\n",
    "data = pd.DataFrame(data, columns=columns)\n",
    "df_sample = data[:10000]\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = [] \n",
    "for i in range(1, 11): \n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 101)\n",
    "    kmeans.fit(df_sample) \n",
    "    wcss.append(kmeans.inertia_)\n",
    "    print(f\"{i} : {kmeans.inertia_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, 11), wcss, 'bo-')\n",
    "plt.xlabel('Values of K')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method using Distortion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the clustering model and visualizer\n",
    "model = KMeans()\n",
    "visualizer = KElbowVisualizer(model, k=(1,13))\n",
    "\n",
    "visualizer.fit(df_sample) # Fit the data to the visualizer\n",
    "visualizer.show() # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MiniBatchKMeans clustering -- in unsupervised learning algorithms\n",
    "\n",
    "kmeans = MiniBatchKMeans(n_clusters=5, random_state=0, batch_size=1000, max_iter=100).fit(data)\n",
    "\n",
    "# n_clusters : number of personality type (in our case its 10 -- you can change it with any number of cluster)\n",
    "# random_state : change as you want\n",
    "# batch_size : the amount of data that is going to train at once or one at a time (feed the data in batches)\n",
    "# max_iter : train the data at n times (in our case its 100 times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check the number of cluster after model train\n",
    "\n",
    "len(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find personality types -- most common answer of each type or common answer pattern\n",
    "\n",
    "one = kmeans.cluster_centers_[0]\n",
    "two = kmeans.cluster_centers_[1]\n",
    "three =kmeans.cluster_centers_[2]\n",
    "four = kmeans.cluster_centers_[3]\n",
    "five =kmeans.cluster_centers_[4]\n",
    "six = kmeans.cluster_centers_[5]\n",
    "seven = kmeans.cluster_centers_[6]\n",
    "eight = kmeans.cluster_centers_[7]\n",
    "nine= kmeans.cluster_centers_[8]\n",
    "ten = kmeans.cluster_centers_[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_types = {'one':one, 'two': two, 'three' :three, 'four':four, 'five':five, 'six': six, 'seven': seven, 'eight': eight,\n",
    "             'nine': nine, 'ten': ten}\n",
    "\n",
    "all_types_scores ={}\n",
    "\n",
    "for name, personality_type in all_types.items():\n",
    "    personality_trait = {}\n",
    "\n",
    "    personality_trait['extroversion_score'] =  personality_type[0] - personality_type[1] + personality_type[2] - personality_type[3] + personality_type[4] - personality_type[5] + personality_type[6] - personality_type[7] + personality_type[8] - personality_type[9]\n",
    "    personality_trait['neuroticism_score'] =  personality_type[0] - personality_type[1] + personality_type[2] - personality_type[3] + personality_type[4] + personality_type[5] + personality_type[6] + personality_type[7] + personality_type[8] + personality_type[9]\n",
    "    personality_trait['agreeableness_score'] =  -personality_type[0] + personality_type[1] - personality_type[2] + personality_type[3] - personality_type[4] - personality_type[5] + personality_type[6] - personality_type[7] + personality_type[8] + personality_type[9]\n",
    "    personality_trait['conscientiousness_score'] = personality_type[0] - personality_type[1] + personality_type[2] - personality_type[3] + personality_type[4] - personality_type[5] + personality_type[6] - personality_type[7] + personality_type[8] + personality_type[9]\n",
    "    personality_trait['openness_score'] =  personality_type[0] - personality_type[1] + personality_type[2] - personality_type[3] + personality_type[4] - personality_type[5] + personality_type[6] + personality_type[7] + personality_type[8] + personality_type[9]\n",
    "    \n",
    "    all_types_scores[name] = personality_trait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_types_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_extroversion = []\n",
    "all_neuroticism =[]\n",
    "all_agreeableness =[]\n",
    "all_conscientiousness =[]\n",
    "all_openness =[]\n",
    "\n",
    "for personality_type, personality_trait in all_types_scores.items():\n",
    "    all_extroversion.append(personality_trait['extroversion_score'])\n",
    "    all_neuroticism.append(personality_trait['neuroticism_score'])\n",
    "    all_agreeableness.append(personality_trait['agreeableness_score'])\n",
    "    all_conscientiousness.append(personality_trait['conscientiousness_score'])\n",
    "    all_openness.append(personality_trait['openness_score'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_extroversion_normalized = (all_extroversion-min(all_extroversion))/(max(all_extroversion)-min(all_extroversion))\n",
    "all_neuroticism_normalized = (all_neuroticism-min(all_neuroticism))/(max(all_neuroticism)-min(all_neuroticism))\n",
    "all_agreeableness_normalized = (all_agreeableness-min(all_agreeableness))/(max(all_agreeableness)-min(all_agreeableness))\n",
    "all_conscientiousness_normalized = (all_conscientiousness-min(all_conscientiousness))/(max(all_conscientiousness)-min(all_conscientiousness))\n",
    "all_openness_normalized = (all_openness-min(all_openness))/(max(all_openness)-min(all_openness))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_extroversion_normalized, len(all_extroversion_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "normalized_all_types_scores ={}\n",
    "\n",
    "for personality_type, personality_trait in all_types_scores.items():\n",
    "    normalized_personality_trait ={}\n",
    "    normalized_personality_trait['extroversion_score'] = all_extroversion_normalized[counter]\n",
    "    normalized_personality_trait['neuroticism_score'] = all_neuroticism_normalized[counter]\n",
    "    normalized_personality_trait['agreeableness_score'] = all_agreeableness_normalized[counter]\n",
    "    normalized_personality_trait['conscientiousness_score'] = all_conscientiousness_normalized[counter]\n",
    "    normalized_personality_trait['openness_score'] = all_openness_normalized[counter]\n",
    "    \n",
    "    normalized_all_types_scores[personality_type] = normalized_personality_trait\n",
    "    \n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_all_types_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in all_types.keys():\n",
    "\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.ylim(0, 1)\n",
    "    plt.bar(list(normalized_all_types_scores[k].keys()), normalized_all_types_scores[k].values(), color='b')\n",
    "    plt.title(f\"Score of personality type '{k.upper()}' \", size=18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = kmeans.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y, len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_labels = pd.DataFrame(X, columns=X.columns)\n",
    "df_with_labels['personality_type'] = Y\n",
    "df_with_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the Cluster Predictions\n",
    "\n",
    "# In order to visualize in 2D graph I will use PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_fit = pca.fit_transform(X)\n",
    "\n",
    "df_pca = pd.DataFrame(data=pca_fit, columns=['PCA1', 'PCA2'])\n",
    "df_pca['Clusters'] = Y\n",
    "df_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.scatterplot(data=df_pca, x='PCA1', y='PCA2', hue='Clusters', palette='Set1', alpha=0.8)\n",
    "plt.title('Personality Clusters after PCA');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the Cluster Predictions\n",
    "\n",
    "# In order to visualize in 2D graph I will use PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "pca_fit = pca.fit_transform(X)\n",
    "\n",
    "df_pca = pd.DataFrame(data=pca_fit, columns=['PCA1', 'PCA2', 'PCA3'])\n",
    "df_pca['Clusters'] = Y\n",
    "df_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca_1 = df_pca[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.scatter_3d(df_pca_1, x='PCA1', y='PCA2', z='PCA3', color='Clusters')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the Model to See My Personality\n",
    "columns = ['EXT1', 'EXT2', 'EXT3', 'EXT4', 'EXT5', 'EXT6', 'EXT7', 'EXT8', 'EXT9', 'EXT10', \n",
    "           'EST1', 'EST2', 'EST3', 'EST4', 'EST5', 'EST6', 'EST7', 'EST8', 'EST9', 'EST10', \n",
    "           'AGR1', 'AGR2', 'AGR3', 'AGR4', 'AGR5', 'AGR6', 'AGR7', 'AGR8', 'AGR9', 'AGR10', \n",
    "           'CSN1', 'CSN2', 'CSN3', 'CSN4', 'CSN5', 'CSN6', 'CSN7', 'CSN8', 'CSN9', 'CSN10', \n",
    "           'OPN1', 'OPN2', 'OPN3', 'OPN4', 'OPN5', 'OPN6', 'OPN7', 'OPN8', 'OPN9', 'OPN10']\n",
    "\n",
    "val1 = [random.randint(0,5) for ind in range(10)]\n",
    "val2 = [random.randint(0,5) for ind in range(10)]\n",
    "val3 = [random.randint(0,5) for ind in range(10)]\n",
    "val4 = [random.randint(0,5) for ind in range(10)]\n",
    "val5 = [random.randint(0,5) for ind in range(10)]\n",
    "val = val1+val2+val3+val4+val5\n",
    "len(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = pd.DataFrame(data=[val], columns=columns)\n",
    "my_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data1 = scaler.transform(my_data)\n",
    "my_data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_personality = kmeans.predict(my_data1)\n",
    "print('My Personality Type Cluster is : ', my_personality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summing up the my question groups\n",
    "col_list = list(my_data)\n",
    "\n",
    "# ext = col_list[0:10]\n",
    "# est = col_list[10:20]\n",
    "# agr = col_list[20:30]\n",
    "# csn = col_list[30:40]\n",
    "# opn = col_list[40:50]\n",
    "\n",
    "\n",
    "ext = list(my_data1[0][0:10])\n",
    "est = list(my_data1[0][10:20])\n",
    "agr = list(my_data1[0][20:30])\n",
    "csn = list(my_data1[0][30:40])\n",
    "opn = list(my_data1[0][40:50])\n",
    "\n",
    "extroversion = round(ext[0] - ext[1] + ext[2] - ext[3] + ext[4] - ext[5] + ext[6] - ext[7] + ext[8] - ext[9], 2)\n",
    "neurotic = round(est[0] - est[1] + est[2] - est[3] + est[4] + est[5] + est[6] + est[7] + est[8] + est[9], 2)\n",
    "agreeable = round(-agr[0] + agr[1] - agr[2] + agr[3] - agr[4] - agr[5] + agr[6] - agr[7] + agr[8] + agr[9], 2)\n",
    "conscientious = round(csn[0] - csn[1] + csn[2] - csn[3] + csn[4] - csn[5] + csn[6] - csn[7] + csn[8] + csn[9], 2)\n",
    "open_ = round(opn[0] - opn[1] + opn[2] - opn[3] + opn[4] - opn[5] + opn[6] + opn[7] + opn[8] + opn[9], 2)\n",
    "\n",
    "li = [extroversion, neurotic, agreeable, conscientious, open_]\n",
    "scaled_data = (li - min(li)) / (max(li) - min(li))\n",
    "\n",
    "my_sums = pd.DataFrame([scaled_data], \n",
    "                       columns=['extroversion', 'neurotic', 'agreeable', 'conscientious', 'open'])\n",
    "\n",
    "my_sums['cluster'] = my_personality\n",
    "\n",
    "print('Sum of my question groups')\n",
    "my_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "# plt.ylim(0, 1)\n",
    "x_ax = my_sums.columns[:-1]\n",
    "y_ax = my_sums.values[0][:-1]\n",
    "plt.bar(x_ax, y_ax, color='b')\n",
    "plt.title(f\"Score of your personality type \", size=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save the MinMaxScaler to disk\n",
    "filename = 'MinMaxScaler_for_personality_type.pkl'\n",
    "pickle.dump(scaler, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the Module to disk\n",
    "filename = 'personality_type_model.pkl'\n",
    "pickle.dump(kmeans, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_scaler = pickle.load(open('MinMaxScaler_for_personality_type.pkl', 'rb')) \n",
    "loaded_model = pickle.load(open('personality_type_model.pkl', 'rb')) \n",
    "\n",
    "# Implementing the Model to See My Personality\n",
    "columns = ['EXT1', 'EXT2', 'EXT3', 'EXT4', 'EXT5', 'EXT6', 'EXT7', 'EXT8', 'EXT9', 'EXT10', \n",
    "           'EST1', 'EST2', 'EST3', 'EST4', 'EST5', 'EST6', 'EST7', 'EST8', 'EST9', 'EST10', \n",
    "           'AGR1', 'AGR2', 'AGR3', 'AGR4', 'AGR5', 'AGR6', 'AGR7', 'AGR8', 'AGR9', 'AGR10', \n",
    "           'CSN1', 'CSN2', 'CSN3', 'CSN4', 'CSN5', 'CSN6', 'CSN7', 'CSN8', 'CSN9', 'CSN10', \n",
    "           'OPN1', 'OPN2', 'OPN3', 'OPN4', 'OPN5', 'OPN6', 'OPN7', 'OPN8', 'OPN9', 'OPN10']\n",
    "\n",
    "val1 = [random.randint(0,2) for ind in range(10)]\n",
    "val2 = [random.randint(0,2) for ind in range(10)]\n",
    "val3 = [random.randint(0,2) for ind in range(10)]\n",
    "val4 = [random.randint(0,2) for ind in range(10)]\n",
    "val5 = [random.randint(0,2) for ind in range(10)]\n",
    "val = val1+val2+val3+val4+val5\n",
    "print('length of val : ',len(val), val)\n",
    "\n",
    "my_data = pd.DataFrame(data=[val], columns=columns)\n",
    "my_data1 = loaded_scaler.transform(my_data)\n",
    "\n",
    "my_personality = loaded_model.predict(my_data1)\n",
    "print('My Personality Type Cluster is : ', my_personality)\n",
    "\n",
    "# Summing up the my question groups\n",
    "col_list = list(my_data)\n",
    "\n",
    "ext = list(my_data1[0][0:10])\n",
    "est = list(my_data1[0][10:20])\n",
    "agr = list(my_data1[0][20:30])\n",
    "csn = list(my_data1[0][30:40])\n",
    "opn = list(my_data1[0][40:50])\n",
    "\n",
    "extroversion = ext[0] - ext[1] + ext[2] - ext[3] + ext[4] - ext[5] + ext[6] - ext[7] + ext[8] - ext[9]\n",
    "neurotic = est[0] - est[1] + est[2] - est[3] + est[4] + est[5] + est[6] + est[7] + est[8] + est[9]\n",
    "agreeable = -agr[0] + agr[1] - agr[2] + agr[3] - agr[4] - agr[5] + agr[6] - agr[7] + agr[8] + agr[9]\n",
    "conscientious = csn[0] - csn[1] + csn[2] - csn[3] + csn[4] - csn[5] + csn[6] - csn[7] + csn[8] + csn[9]\n",
    "open_ = opn[0] - opn[1] + opn[2] - opn[3] + opn[4] - opn[5] + opn[6] + opn[7] + opn[8] + opn[9]\n",
    "\n",
    "li = [extroversion, neurotic, agreeable, conscientious, open_]\n",
    "scaled_data = (li - min(li)) / (max(li) - min(li))\n",
    "\n",
    "my_sums = pd.DataFrame([scaled_data], \n",
    "                       columns=['extroversion', 'neurotic', 'agreeable', 'conscientious', 'open'])\n",
    "\n",
    "my_sums['cluster'] = my_personality\n",
    "\n",
    "print('Sum of my question groups')\n",
    "my_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "# plt.ylim(0, 1)\n",
    "x_ax = my_sums.columns[:-1]\n",
    "y_ax = my_sums.values[0][:-1]\n",
    "plt.bar(x_ax, y_ax, color='b')\n",
    "plt.title(f\"Score of your personality type \", size=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumeDataSet = pd.read_csv('dataset/UpdatedResumeDataSet.csv' ,encoding='utf-8')\n",
    "resumeDataSet['cleaned_resume'] = ''\n",
    "resumeDataSet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Displaying the distinct categories of resume -\")\n",
    "print (resumeDataSet['Category'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Displaying the distinct categories of resume and the number of records belonging to each category -\")\n",
    "print (resumeDataSet['Category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.xticks(rotation=90)\n",
    "sns.countplot(y=\"Category\", data=resumeDataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to eliminate regular expression\n",
    "\n",
    "import re\n",
    "def cleanResume(resumeText):\n",
    "    resumeText = re.sub('http\\S+\\s*', ' ', resumeText)  # remove URLs\n",
    "    resumeText = re.sub('RT|cc', ' ', resumeText)  # remove RT and cc\n",
    "    resumeText = re.sub('#\\S+', '', resumeText)  # remove hashtags\n",
    "    resumeText = re.sub('@\\S+', '  ', resumeText)  # remove mentions\n",
    "    resumeText = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', resumeText)  # remove punctuations\n",
    "    resumeText = re.sub(r'[^\\x00-\\x7f]',r' ', resumeText) \n",
    "    resumeText = re.sub('\\s+', ' ', resumeText)  # remove extra whitespace\n",
    "    return resumeText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumeDataSet['cleaned_resume'] = resumeDataSet.Resume.apply(lambda x: cleanResume(x))\n",
    "print (resumeDataSet['cleaned_resume'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# !pip install wordcloud\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "oneSetOfStopWords = set(stopwords.words('english')+['``',\"''\"])\n",
    "totalWords =[]\n",
    "Sentences = resumeDataSet['Resume'].values\n",
    "print('Sentences : \\n',Sentences[0],'\\n')\n",
    "cleanedSentences = \"\"\n",
    "for i in range(0,160):\n",
    "    cleanedText = cleanResume(Sentences[i])\n",
    "    cleanedSentences += cleanedText\n",
    "    requiredWords = nltk.word_tokenize(cleanedText)\n",
    "    for word in requiredWords:\n",
    "        if word not in oneSetOfStopWords and word not in string.punctuation:\n",
    "            totalWords.append(word)\n",
    "    \n",
    "wordfreqdist = nltk.FreqDist(totalWords)\n",
    "mostcommon = wordfreqdist.most_common(50)\n",
    "print('mostcommon : ', mostcommon,'\\n')\n",
    "print('cleanedSentences : ', cleanedSentences,'\\n')\n",
    "\n",
    "wc = WordCloud().generate(cleanedSentences)\n",
    "print('wc : ', wc,'\\n')\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(wc, interpolation='spline16') # bilinear\n",
    "# 'antialiased', 'none', 'nearest', 'bilinear', 'bicubic', 'spline16', 'spline36', \n",
    "# 'hanning', 'hamming', 'hermite', 'kaiser', 'quadric', 'catrom', 'gaussian', 'bessel', \n",
    "# 'mitchell', 'sinc', 'lanczos', 'blackman'\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform target variable using label encoding\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "var_mod = ['Category']\n",
    "le = LabelEncoder()\n",
    "for i in var_mod:\n",
    "    resumeDataSet[i] = le.fit_transform(resumeDataSet[i])\n",
    "print (\"CONVERTED THE CATEGORICAL VARIABLES INTO NUMERICALS\")\n",
    "print('resumeDataSet : ')\n",
    "resumeDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumeDataSet.to_csv('resumeDataSet.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text representation using TFIDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "requiredText = resumeDataSet['cleaned_resume'].values\n",
    "requiredTarget = resumeDataSet['Category'].values\n",
    "print('requiredText : ',requiredText,'\\n')\n",
    "print('requiredTarget : ',requiredTarget,'\\n')\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    stop_words='english',\n",
    "    max_features=1500)\n",
    "word_vectorizer.fit(requiredText)\n",
    "WordFeatures = word_vectorizer.transform(requiredText)\n",
    "print('WordFeatures : ',WordFeatures,'\\n')\n",
    "\n",
    "print (\"Feature completed .....\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the date\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(WordFeatures,requiredTarget,random_state=0, test_size=0.2,stratify=requiredTarget)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = OneVsRestClassifier(KNeighborsClassifier())\n",
    "clf.fit(X_train, y_train)\n",
    "prediction = clf.predict(X_test)\n",
    "\n",
    "print('Accuracy of KNeighbors Classifier on training set: {:.2f}'.format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of KNeighbors Classifier on test set: {:.2f}'.format(clf.score(X_test, y_test)))\n",
    "print(\"\\n Classification report for classifier %s:\\n%s\\n\" % (clf, metrics.classification_report(y_test, prediction)))\n",
    "#print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using our CVs to test the model\n",
    "\n",
    "Test_CVs = pd.read_csv('dataset/resume_dataset.csv' ,encoding='utf-8')\n",
    "Test_CVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_CVs['cleaned_resume'] = ''\n",
    "Test_CVs['cleaned_resume']= Test_CVs.Resume.apply(lambda x: cleanResume(x))\n",
    "Test_CVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Test_CVs['cleaned_resume'][0],'\\n', len(Test_CVs['cleaned_resume'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = Test_CVs['cleaned_resume'].values\n",
    "\n",
    "# word_vectorizer = TfidfVectorizer(\n",
    "#     sublinear_tf=True,\n",
    "#     stop_words='english',\n",
    "#     max_features=1500)\n",
    "# word_vectorizer.fit(test_text)\n",
    "\n",
    "WordFeatures = word_vectorizer.transform(test_text)\n",
    "print('WordFeatures : ', WordFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordFeatures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test2=WordFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = clf.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction= le.inverse_transform(y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_resume = '''\n",
    "Mohamed Abdelghani Mohamed Mobile 20 0101 024 336 5 mohamedabdelghani1511 Objective Strong analytical thinker with high \n",
    "problem solving and communication skills Seeking an opportunity in data science and machine learning field to utilize my \n",
    "data science skills to transform data into business value Education 9 month Diploma Information Technology Institute Smart \n",
    "Village Oct 2020 Present Track Date Science Intake 41 Graduation Project Resume Ranking using NLP for a multinational \n",
    "telecommunication company Bachelor of Engineering Ain Shams University Year of graduation 2017 Department Electrical \n",
    "Engineering Grade Very Good Work Experience Senior technical support engineer at Orange Business Services Sep 2018 Dec \n",
    "2020 Providing a professional technical point of contact for customers for different services including L3 VPN solution \n",
    "VPN remote access Z scalar proxy Skype for business Acting as an escalation manager for chronic and complex problems and \n",
    "incidents Awarded many local awards for performance excellence Acting as a shift leader and conduct trainings to the new \n",
    "comers Intern Orange Business Services got introduced to Orange Support functions August 2017 Intern Schneider Electric \n",
    "got introduced to Schneider products and services July 2016 Intern ABB got assigned to the tendering department August \n",
    "2015 Skills Technical Skills Concepts Machine Learning Cloud computing Fundamentals Business Statistics Data warehouse \n",
    "fundamentals and Data modeling Deep learning Agile Methodologies Visualization and story telling Big Data fundamentals \n",
    "Optimization and Simulation methods Modeling and Operations research OOP Systems Thinking Tools Programming Languages \n",
    "Python R SAS Bash Java Database SQL Oracle PL SQL Analytical SQL Tools Excel Hadoop Spark Linux Languages Skills Fluent in \n",
    "spoken and written English Arabic as a native speaker Very good command of written and spoken French Extracurricular \n",
    "Activities Business Development Moderator ACES Aug 2016 Jun 2017 Prepared and conducted 30 hrs Business development \n",
    "workshop Participant in Exxon Mobil case study MECA Academy Sep 2016 Jun 2017 Awarded special mention perseverance in \n",
    "difficult circumstances from Exxon Mobil Academic committee member Pirates Egypt Aug 2014 Jun 2015 Prepared and conducted \n",
    "30 hrs CCNA workshop Projects Recommender system using XGBoost for Airbnb use case Energy consumption prediction using \n",
    "LightGBM Movie rating prediction using Random Forest Analyzing ticketing system performance using R Certificates Machine \n",
    "learning course offered by Stanford University Coursera Feb 2021 ITILV3 Dec 2019 Six Sigma Yellow Belt July 2019\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = [test_resume] # Test_CVs['cleaned_resume'].values\n",
    "\n",
    "# word_vectorizer = TfidfVectorizer(\n",
    "#     sublinear_tf=True,\n",
    "#     stop_words='english',\n",
    "#     max_features=1500)\n",
    "# word_vectorizer.fit(test_text)\n",
    "\n",
    "WordFeatures = word_vectorizer.transform(test_text)\n",
    "print('WordFeatures : ', WordFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordFeatures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test2=WordFeatures\n",
    "y_pred2 = clf.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction= le.inverse_transform(y_pred2)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "import pickle\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'Resume_Classification_KNN.pkl'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open('Resume_Classification_KNN.pkl', 'rb')) \n",
    "\n",
    "y_pred = loaded_model.predict(X_test2)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction= le.inverse_transform(y_pred)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
