{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "384da6a4",
   "metadata": {},
   "source": [
    "1. Convolutional layers with Batch Normalization and Dropout.\n",
    "2. Attention Mechanism: We'll integrate a basic channel-wise attention mechanism.\n",
    "3. Advanced layers: Using layers like Separable Convolutions, and GlobalAveragePooling for more efficient feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ef1aaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProjectWork\\Basic_Python\\Suresh Interview Project\\Project_Multi_Cancer\\venv_multican\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProjectWork\\Basic_Python\\Suresh Interview Project\\Project_Multi_Cancer\\venv_multican\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProjectWork\\Basic_Python\\Suresh Interview Project\\Project_Multi_Cancer\\venv_multican\\lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProjectWork\\Basic_Python\\Suresh Interview Project\\Project_Multi_Cancer\\venv_multican\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 128, 128, 3)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 128, 128, 32)      896       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 128, 128, 32)      128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 128, 128, 32)      9248      \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 128, 128, 32)      128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " channel_attention (Channel  (None, 128, 128, 32)      292       \n",
      " Attention)                                                      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 64, 64, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64, 64, 32)        0         \n",
      "                                                                 \n",
      " separable_conv2d (Separabl  (None, 64, 64, 64)        2400      \n",
      " eConv2D)                                                        \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 64, 64, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " separable_conv2d_1 (Separa  (None, 64, 64, 64)        4736      \n",
      " bleConv2D)                                                      \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 64, 64, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " channel_attention_1 (Chann  (None, 64, 64, 64)        1096      \n",
      " elAttention)                                                    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 32, 32, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 32, 32, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 32, 32, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 32, 32, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 32, 32, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " channel_attention_2 (Chann  (None, 32, 32, 128)       4240      \n",
      " elAttention)                                                    \n",
      "                                                                 \n",
      " global_average_pooling2d_3  (None, 128)               0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 263942 (1.01 MB)\n",
      "Trainable params: 263046 (1.00 MB)\n",
      "Non-trainable params: 896 (3.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Custom Channel-wise Attention Mechanism\n",
    "class ChannelAttention(layers.Layer):\n",
    "    def __init__(self, channels, reduction_ratio=8):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.reduction_ratio = reduction_ratio\n",
    "        self.global_avg_pool = layers.GlobalAveragePooling2D()\n",
    "        self.global_max_pool = layers.GlobalMaxPooling2D()\n",
    "        \n",
    "        self.fc1 = layers.Dense(channels // reduction_ratio, activation='relu')\n",
    "        self.fc2 = layers.Dense(channels)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        avg_pool = self.global_avg_pool(inputs)\n",
    "        max_pool = self.global_max_pool(inputs)\n",
    "        \n",
    "        avg_pool = layers.Reshape((1, 1, self.channels))(avg_pool)\n",
    "        max_pool = layers.Reshape((1, 1, self.channels))(max_pool)\n",
    "        \n",
    "        avg_fc = self.fc2(self.fc1(avg_pool))\n",
    "        max_fc = self.fc2(self.fc1(max_pool))\n",
    "        \n",
    "        attention = layers.Activation('sigmoid')(avg_fc + max_fc)\n",
    "        \n",
    "        return inputs * attention\n",
    "\n",
    "# Custom CNN with Attention\n",
    "def custom_cnn_with_attention(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Block 1: Convolutional block with Attention\n",
    "    x = layers.Conv2D(32, (3, 3), padding='same', activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = ChannelAttention(32)(x)                                                 # Attention applied here\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    # Block 2: Advanced Convolutional Block (Separable Conv)\n",
    "    x = layers.SeparableConv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.SeparableConv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = ChannelAttention(64)(x)                                                 # Attention applied here\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "\n",
    "    # Block 3: Deeper Conv Block with Global Pooling\n",
    "    x = layers.Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = ChannelAttention(128)(x)                                                # Attention applied here\n",
    "    \n",
    "    # Global Average Pooling and Fully Connected Layers\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Model Compilation and Summary\n",
    "input_shape = (128, 128, 3)  # Example input shape\n",
    "num_classes = 10  # Example number of output classes\n",
    "\n",
    "model = custom_cnn_with_attention(input_shape, num_classes)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e4f4fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
