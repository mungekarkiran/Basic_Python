{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97c03d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProjectWork\\Basic_Python\\Suresh Interview Project\\Project_Multi_Cancer\\venv_multican\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten, BatchNormalization\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from keras.applications import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f8b08c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10798 images belonging to 2 classes.\n",
      "Found 7200 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data_path = os.path.join('..', 'Dataset', 'Binary_class_dataset', 'train') # '/content/data/train'\n",
    "valid_data_path = os.path.join('..', 'Dataset', 'Binary_class_dataset', 'test') # '/content/data/test'\n",
    "\n",
    "train_data_agumentation = ImageDataGenerator(rescale = 1./255,\n",
    "                                             zoom_range = 0.1\n",
    "                                             )\n",
    "\n",
    "val_data_agumentation = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "\n",
    "# load training data\n",
    "train_data = train_data_agumentation.flow_from_directory(directory = train_data_path,\n",
    "                                                         target_size = (224,224),\n",
    "                                                         class_mode = 'categorical',\n",
    "                                                         batch_size = 8)\n",
    "\n",
    "val_data = val_data_agumentation.flow_from_directory(directory = valid_data_path,\n",
    "                                                     target_size = (224,224),\n",
    "                                                     class_mode = 'categorical',\n",
    "                                                     batch_size = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e1f56bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = 'checkpoints_Binary_class_vgg16_adam'\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "checkpoint_filepath = os.path.join(checkpoint_dir,\n",
    "                                   \"model_epoch_{epoch:02d}_val_acc_{val_accuracy:.2f}_val_loss_{val_loss:.2f}.keras\")\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath = checkpoint_filepath,\n",
    "                             monitor = 'val_loss',\n",
    "                             verbose = 0,\n",
    "                             save_best_only = True,\n",
    "                             save_weights_only = False,\n",
    "                             mode = 'auto')\n",
    "\n",
    "early = EarlyStopping(monitor = 'val_loss',\n",
    "                      min_delta = 0,\n",
    "                      patience = 10,\n",
    "                      verbose = 0,\n",
    "                      mode = 'auto')\n",
    "\n",
    "reduceLR = ReduceLROnPlateau(monitor = \"val_loss\", \n",
    "                             factor = 0.1,\n",
    "                             patience = 5, \n",
    "                             verbose = 0, \n",
    "                             mode = \"auto\",\n",
    "                             min_delta = 0.0001, \n",
    "                             cooldown = 0,\n",
    "                             min_lr = 0.0)\n",
    "\n",
    "csv_logger = CSVLogger(os.path.join(checkpoint_dir, 'training.log'))\n",
    "\n",
    "callbacks_list = [checkpoint, reduceLR, csv_logger] #, early\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2741f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProjectWork\\Basic_Python\\Suresh Interview Project\\Project_Multi_Cancer\\venv_multican\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProjectWork\\Basic_Python\\Suresh Interview Project\\Project_Multi_Cancer\\venv_multican\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 50178     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14764866 (56.32 MB)\n",
      "Trainable params: 50178 (196.01 KB)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "image_size = [224, 224]\n",
    "num_classes = len(glob.glob(train_data_path+'/*'))\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = VGG16(input_shape = image_size + [3],\n",
    "              weights = 'imagenet',\n",
    "              include_top = False)\n",
    "\n",
    "# our layers - you can add more if you want\n",
    "x = Flatten()(model.output)\n",
    "\n",
    "output_layer = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# model.layers.trainable = False\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# create a model object\n",
    "model = Model(inputs = model.input, outputs = output_layer)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5998d332",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the hyperparameters\n",
    "adam = Adam(learning_rate = 0.001, \n",
    "            beta_1 = 0.9,\n",
    "            beta_2 = 0.999,\n",
    "            epsilon = 1e-07)\n",
    "\n",
    "# compile the model with adam optimizer, categorical_croosentropy loss function\n",
    "# model.compile(optimizer = 'adam',\n",
    "#               loss = 'categorical_crossentropy',\n",
    "#               metrics = ['accuracy'])\n",
    "model.compile(optimizer = adam,\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc559105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:From C:\\ProjectWork\\Basic_Python\\Suresh Interview Project\\Project_Multi_Cancer\\venv_multican\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProjectWork\\Basic_Python\\Suresh Interview Project\\Project_Multi_Cancer\\venv_multican\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1350/1350 [==============================] - 4389s 3s/step - loss: 0.7146 - accuracy: 0.7786 - val_loss: 0.5960 - val_accuracy: 0.8111 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "1350/1350 [==============================] - 4716s 3s/step - loss: 0.5771 - accuracy: 0.8242 - val_loss: 0.5235 - val_accuracy: 0.8250 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "1350/1350 [==============================] - ETA: 0s - loss: 0.5259 - accuracy: 0.8393"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data,\n",
    "                    batch_size = 8,\n",
    "                    epochs = 5,\n",
    "                    validation_data = val_data,\n",
    "                    callbacks = callbacks_list)\n",
    "\n",
    "# steps_per_epoch=len(training_set),\n",
    "# validation_steps=len(test_set),\n",
    "\n",
    "# Save the trained model\n",
    "save_model_path = os.path.join(checkpoint_dir, 'Binary_class_VGG16_adam_model_last_epoch.h5')\n",
    "model.save(save_model_path)\n",
    "print(f\"Model saved as {save_model_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b609b6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracies\n",
    "plt.plot(history.history['accuracy'], label='train accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('Binary_class_VGG16_adam_model_accuracy_and_val_accuracy.png', dpi=200)\n",
    "plt.show()\n",
    " \n",
    "# loss\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='val loss')\n",
    "plt.legend()\n",
    "plt.savefig('Binary_class_VGG16_adam_model_loss_and_val_loss.png', dpi=200)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df99d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(train_data, verbose=0)\n",
    "print('Train loss:', score[0])\n",
    "print('Train accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210b92a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(val_data, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5189022",
   "metadata": {},
   "source": [
    "# Create Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f766152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e613d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Classification Report\n",
    "\n",
    "def evaluate_model(y_true, y_pred, class_names=None):\n",
    "    \"\"\"\n",
    "    This function evaluates the performance of a model and prints the confusion matrix, \n",
    "    accuracy score, classification report, and Cohen's kappa score. It also plots a heatmap of the confusion matrix.\n",
    "    \n",
    "    Args:\n",
    "        y_true (array-like): Ground truth (true labels)\n",
    "        y_pred (array-like): Predicted labels from the model\n",
    "        class_names (list): List of class names for better readability in the confusion matrix\n",
    "        \n",
    "    Returns:\n",
    "        None: Displays the evaluation metrics and heatmap.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Calculate accuracy score\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Get classification report\n",
    "    class_report = classification_report(y_true, y_pred, target_names=class_names)\n",
    "    \n",
    "    # Calculate Cohen's kappa score\n",
    "    kappa_score = cohen_kappa_score(y_true, y_pred)\n",
    "    \n",
    "    # Print evaluation metrics\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    print(f\"\\nAccuracy Score: {accuracy * 100:.2f}%\")\n",
    "    print(\"\\nClassification Report:\\n\", class_report)\n",
    "    print(f\"Cohen's Kappa Score: {kappa_score:.4f}\")\n",
    "    \n",
    "    # Plot heatmap for confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix Heatmap')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_classification_report(model_path, test_data_dir):\n",
    "    # Step 1: Set up directories and parameters\n",
    "    # test_data_dir = 'data/train/'  # Path to the directory containing test images in folders 0, 1, 2\n",
    "\n",
    "    # Step 2: Load the trained model\n",
    "    model = load_model(model_path)  # Assuming you've saved the model as .h5\n",
    "\n",
    "    # Step 3: ImageDataGenerator for loading test images (no augmentation needed for testing)\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)  # Rescale pixel values to [0, 1]\n",
    "\n",
    "    # Step 4: Load test data using flow_from_directory\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size = (224, 224),  # Same size as the input size for your model\n",
    "        batch_size = 32,  # Adjust according to your hardware\n",
    "        class_mode = 'categorical',  # For multi-class classification\n",
    "        shuffle = False  # Don't shuffle, we need to keep track of the order for y_true\n",
    "    )\n",
    "\n",
    "    # Step 5: Get true labels from the generator\n",
    "    y_true = test_generator.classes  # These are the true class labels\n",
    "\n",
    "    # Step 6: Predict using the model\n",
    "    y_pred_prob = model.predict(test_generator)  # Predict probabilities for each class\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)  # Get the index of the max probability (class label)\n",
    "\n",
    "    # Step 7: Map predicted and true labels to class names\n",
    "    class_labels = list(test_generator.class_indices.keys())\n",
    "\n",
    "    # You can use the evaluation function from earlier for further analysis\n",
    "    evaluate_model(y_true, y_pred, class_names=class_labels)\n",
    "    \n",
    "# Call the function\n",
    "\n",
    "model_path = os.path.join(checkpoint_dir, 'Binary_class_VGG16_adam_model_last_epoch.h5')\n",
    "test_data_dir = os.path.join('..', 'Dataset', 'Binary_class_dataset', 'val')\n",
    "\n",
    "create_classification_report(model_path, test_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f1bc55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e025640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b31bddf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
