{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96ad541a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProjectWork\\Basic_Python\\Suresh Interview Project\\Project_Multi_Cancer\\venv_multican\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lib's\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "082ac841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Mechanism - Squeeze and Excitation Block\n",
    "def squeeze_excite_block(input_tensor, ratio=16):\n",
    "    filters = input_tensor.shape[-1]\n",
    "    se_shape = (1, 1, filters)\n",
    "\n",
    "    se = layers.GlobalAveragePooling2D()(input_tensor)\n",
    "    se = layers.Reshape(se_shape)(se)\n",
    "    se = layers.Dense(filters // ratio, activation='relu', use_bias=False)(se)\n",
    "    se = layers.Dense(filters, activation='sigmoid', use_bias=False)(se)\n",
    "\n",
    "    x = layers.multiply([input_tensor, se])\n",
    "    return x\n",
    "\n",
    "# Residual Block with Attention Mechanism\n",
    "def residual_block(x, filters, kernel_size=3, stride=1, se_block=False):\n",
    "    shortcut = x\n",
    "    \n",
    "    # First Convolution\n",
    "    x = layers.Conv2D(filters, kernel_size, padding='same', strides=stride)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    # Second Convolution\n",
    "    x = layers.Conv2D(filters, kernel_size, padding='same', strides=1)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # Optional Attention Mechanism\n",
    "    if se_block:\n",
    "        x = squeeze_excite_block(x)\n",
    "    \n",
    "    print(x.shape)\n",
    "    print(shortcut.shape)\n",
    "    # Adding shortcut (Residual Connection)\n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Custom CNN Model with Attention Mechanisms and Advanced Layers\n",
    "def custom_cnn_with_SE_attention(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Initial Convolution Block\n",
    "    x = layers.Conv2D(32, (3, 3), padding='same', strides=1)(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    # First Residual Block with SE Attention\n",
    "    x = residual_block(x, filters=64, kernel_size=3, stride=1, se_block=True)\n",
    "    \n",
    "    # Second Residual Block with SE Attention\n",
    "    x = residual_block(x, filters=128, kernel_size=3, stride=1, se_block=True)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    # Third Residual Block without SE Attention\n",
    "    x = residual_block(x, filters=256, kernel_size=3, stride=1, se_block=False)\n",
    "    \n",
    "    # Global Average Pooling for Feature Extraction\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Fully Connected Layer with Dropout\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    # Output Layer (for example, for 3-class classification)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    # Build the Model\n",
    "    model = models.Model(inputs, outputs)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e99990b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1206 images belonging to 3 classes.\n",
      "Found 594 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define the paths for training and validation data\n",
    "train_data_dir = os.path.join('..', 'Dataset', 'data', 'train')  # Root folder that contains subfolders '0', '1', '2'\n",
    "test_data_dir = os.path.join('..', 'Dataset', 'data', 'test')  # Root folder that contains subfolders '0', '1', '2'\n",
    "\n",
    "# Step 2: ImageDataGenerator for loading and augmenting images\n",
    "# You can apply real-time augmentation if needed\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255, \n",
    "    zoom_range = 0.1\n",
    ")\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255 )\n",
    "\n",
    "# Step 3: Load training data (80%) and validation data (20%)\n",
    "# The flow_from_directory function will infer the labels based on the folder names\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory = train_data_dir,\n",
    "    target_size = (128, 128),  # Resize all images to the input shape expected by the model\n",
    "    batch_size = 32,\n",
    "    class_mode = 'categorical'  # For multi-class classification\n",
    ")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    directory = test_data_dir,\n",
    "    target_size = (128, 128),  # Resize all images to the input shape expected by the model\n",
    "    batch_size = 32,\n",
    "    class_mode = 'categorical'  # For multi-class classification\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a9096b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = 'checkpoints_CNN_SE_attention_mechanisms_adam'\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "checkpoint_filepath = os.path.join(checkpoint_dir,\n",
    "                                   \"model_epoch_{epoch:02d}_val_acc_{val_accuracy:.2f}_val_loss_{val_loss:.2f}.h5\")\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath = checkpoint_filepath,\n",
    "                             monitor = 'val_loss',\n",
    "                             verbose = 0,\n",
    "                             save_best_only = True,\n",
    "                             save_weights_only = False,\n",
    "                             mode = 'auto')\n",
    "\n",
    "early = EarlyStopping(monitor = 'val_loss',\n",
    "                      min_delta = 0,\n",
    "                      patience = 10,\n",
    "                      verbose = 0,\n",
    "                      mode = 'auto')\n",
    "\n",
    "reduceLR = ReduceLROnPlateau(monitor = \"val_loss\", \n",
    "                             factor = 0.1,\n",
    "                             patience = 5, \n",
    "                             verbose = 0, \n",
    "                             mode = \"auto\",\n",
    "                             min_delta = 0.0001, \n",
    "                             cooldown = 0,\n",
    "                             min_lr = 0.0)\n",
    "\n",
    "csv_logger = CSVLogger('training.log')\n",
    "\n",
    "callbacks_list = [checkpoint, reduceLR, csv_logger] #, early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cae211f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 64, 64, 64)\n",
      "(None, 64, 64, 32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Inputs have incompatible shapes. Received shapes (64, 64, 64) and (64, 64, 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m3\u001b[39m)  \u001b[38;5;66;03m# Example input shape for images\u001b[39;00m\n\u001b[0;32m      3\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(train_data_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m))) \u001b[38;5;66;03m# 3\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_cnn_with_SE_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Step 5: Compile the model\u001b[39;00m\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      8\u001b[0m               loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      9\u001b[0m               metrics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[6], line 50\u001b[0m, in \u001b[0;36mcustom_cnn_with_SE_attention\u001b[1;34m(input_shape, num_classes)\u001b[0m\n\u001b[0;32m     47\u001b[0m x \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mMaxPooling2D((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m))(x)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# First Residual Block with SE Attention\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mresidual_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mse_block\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Second Residual Block with SE Attention\u001b[39;00m\n\u001b[0;32m     53\u001b[0m x \u001b[38;5;241m=\u001b[39m residual_block(x, filters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, se_block\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[6], line 34\u001b[0m, in \u001b[0;36mresidual_block\u001b[1;34m(x, filters, kernel_size, stride, se_block)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(shortcut\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Adding shortcut (Residual Connection)\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshortcut\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m x \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mActivation(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m)(x)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mC:\\ProjectWork\\Basic_Python\\Suresh Interview Project\\Project_Multi_Cancer\\venv_multican\\lib\\site-packages\\keras\\src\\layers\\merging\\add.py:92\u001b[0m, in \u001b[0;36madd\u001b[1;34m(inputs, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.layers.add\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd\u001b[39m(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Functional interface to the `tf.keras.layers.Add` layer.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m \n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAdd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProjectWork\\Basic_Python\\Suresh Interview Project\\Project_Multi_Cancer\\venv_multican\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\ProjectWork\\Basic_Python\\Suresh Interview Project\\Project_Multi_Cancer\\venv_multican\\lib\\site-packages\\keras\\src\\layers\\merging\\base_merge.py:74\u001b[0m, in \u001b[0;36m_Merge._compute_elemwise_op_output_shape\u001b[1;34m(self, shape1, shape2)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m j:\n\u001b[1;32m---> 74\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     75\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInputs have incompatible shapes. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     76\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     77\u001b[0m             )\n\u001b[0;32m     78\u001b[0m         output_shape\u001b[38;5;241m.\u001b[39mappend(i)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(output_shape)\n",
      "\u001b[1;31mValueError\u001b[0m: Inputs have incompatible shapes. Received shapes (64, 64, 64) and (64, 64, 32)"
     ]
    }
   ],
   "source": [
    "# Step 4: Create the Custom CNN model with Attention Mechanisms\n",
    "input_shape = (128, 128, 3)  # Example input shape for images\n",
    "num_classes = len(glob.glob(os.path.join(train_data_dir, '*'))) # 3\n",
    "model = custom_cnn_with_SE_attention(input_shape, num_classes)\n",
    "\n",
    "# Step 5: Compile the model\n",
    "model.compile(optimizer = 'adam', \n",
    "              loss = 'categorical_crossentropy', \n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "# Step 6: Train the model\n",
    "history = model.fit(\n",
    "    train_generator,  # Training data\n",
    "    epochs = 100,  # Number of epochs\n",
    "    validation_data = validation_generator,  # Validation data\n",
    "    callbacks = callbacks_list\n",
    ")\n",
    "\n",
    "# Optional: Save the trained model\n",
    "save_model_path = os.path.join(checkpoint_dir, 'custom_cnn_SE_attention_model.h5')\n",
    "model.save(save_model_path)\n",
    "print(f\"Model saved as {save_model_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfa23b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
