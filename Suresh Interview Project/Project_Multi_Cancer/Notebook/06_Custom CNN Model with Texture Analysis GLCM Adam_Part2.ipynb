{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d7ee145",
   "metadata": {},
   "source": [
    "## Steps:\n",
    "\n",
    "1. Custom CNN for Feature Extraction\n",
    "2. Texture Analysis using GLCM\n",
    "3. Saving Features to CSV\n",
    "4. Training an ANN Model\n",
    "5. Model Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "628f9f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProjectWork\\Basic_Python\\Suresh Interview Project\\Project_Multi_Cancer\\venv_multican\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from skimage.feature import greycomatrix, greycoprops\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ae0fa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Custom CNN Model for Feature Extraction\n",
    "def create_custom_cnn(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    model = models.Model(inputs, x)\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "364ce009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Function to calculate GLCM properties\n",
    "def calculate_glcm_features(image, distances, angles):\n",
    "    properties = ['contrast', 'dissimilarity', 'homogeneity', \n",
    "                  'energy', 'correlation', 'ASM']\n",
    "    glcm = greycomatrix(image, \n",
    "                        distances = distances, \n",
    "                        angles = angles, \n",
    "                        symmetric = True, \n",
    "                        normed = True)\n",
    "    feature_vector = []\n",
    "    \n",
    "    for prop in properties:\n",
    "        prop_values = greycoprops(glcm, prop)\n",
    "        feature_vector.extend(prop_values.flatten())\n",
    "    \n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6eb1a715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Fetch CNN features, GLCM features, and save to CSV\n",
    "def extract_features_and_save(images, labels, cnn_model, output_csv, distances, angles):\n",
    "    feature_list = []\n",
    "    for i, image in enumerate(images):\n",
    "        print(f\"Processing image {i+1}/{len(images)}\")\n",
    "        \n",
    "        # CNN Features\n",
    "        image_expanded = np.expand_dims(image, axis=0)  # Expand dimensions for the CNN model\n",
    "        cnn_features = cnn_model.predict(image_expanded).flatten()\n",
    "        \n",
    "        # GLCM Features\n",
    "        image_gray = np.mean(image, axis=-1).astype(np.uint8)  # Convert to grayscale if needed\n",
    "        glcm_features = calculate_glcm_features(image_gray, distances, angles)\n",
    "        \n",
    "        # Combine CNN + GLCM features\n",
    "        combined_features = np.concatenate([cnn_features, glcm_features])\n",
    "        feature_list.append(np.concatenate([combined_features, [labels[i]]]))\n",
    "    \n",
    "    # Create a DataFrame and save to CSV\n",
    "    df = pd.DataFrame(feature_list)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Features saved to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfad8305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create ANN model to train using extracted features\n",
    "def create_ann_model(input_dim, num_class):\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_dim=input_dim),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        #layers.Dense(num_class, activation='sigmoid')  # Adjust depending on the classification task (binary/multi-class)\n",
    "        layers.Dense(num_class, activation='softmax')  # Adjust depending on the classification task (binary/multi-class)\n",
    "    ])\n",
    "    \n",
    "    # model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])  # Adjust loss if multi-class\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  # Adjust loss if multi-class\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55f0e540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Function to evaluate ANN model\n",
    "def evaluate_model(model, X_test, y_test, class_names=None):\n",
    "    #y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")  # Binary thresholding for binary classification\n",
    "    #print(classification_report(y_test, y_pred))\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)  # Get class with highest probability\n",
    "    y_true = np.argmax(y_test, axis=1)  # Convert one-hot to labels for classification report\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Calculate accuracy score\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Get classification report\n",
    "    class_report = classification_report(y_true, y_pred, target_names=class_names)\n",
    "    \n",
    "    # Calculate Cohen's kappa score\n",
    "    kappa_score = cohen_kappa_score(y_true, y_pred)\n",
    "    \n",
    "    # Print evaluation metrics\n",
    "    print(f\"\\nAccuracy Score: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Loss Score: {(1- accuracy) * 100:.2f}%\")\n",
    "    print(f\"Cohen's Kappa Score: {kappa_score:.4f}\")\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    print(\"\\nClassification Report:\\n\", class_report)\n",
    "    \n",
    "    # Plot heatmap for confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    try:\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    except Exception as e:\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix Heatmap')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.show()\n",
    "    \n",
    "#     print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8131ff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Load your dataset\n",
    "def load_dataset_from_directory(directory, img_size=(224, 224), batch_size=32):\n",
    "    dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory,\n",
    "        labels = 'inferred',\n",
    "        label_mode = 'int',  # 'int' for integer labels, change to 'categorical' if needed\n",
    "        image_size = img_size,  # Resize all images to the target size\n",
    "        batch_size = batch_size,\n",
    "        shuffle = True\n",
    "    )\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70c9e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 6: Load your dataset\n",
    "    # Load training data\n",
    "    train_directory = os.path.join('..', 'Dataset', 'data', 'train')\n",
    "    train_dataset = load_dataset_from_directory(train_directory)\n",
    "    \n",
    "    # Extract images and labels from the dataset\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for batch in train_dataset:\n",
    "        imgs, lbls = batch\n",
    "        images.append(imgs.numpy())  # Convert to numpy arrays\n",
    "        labels.append(lbls.numpy())\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    images = np.concatenate(images, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    \n",
    "    print(f\"Loaded {images.shape[0]} images of shape {images.shape[1:]}\")\n",
    "    print(f\"Loaded {labels.shape[0]} labels\")\n",
    "\n",
    "#     # Step 7: CNN Model for feature extraction\n",
    "#     cnn_model = create_custom_cnn(input_shape=(128, 128, 3))\n",
    "#     cnn_model.summary()\n",
    "    \n",
    "#     # Step 8: GLCM parameters\n",
    "#     distances = [1, 3, 5, 3, 1, 3, 5]\n",
    "#     angles = [0, 0, 0, np.pi/4, np.pi/2, np.pi/2, np.pi/2]\n",
    "\n",
    "#     # Step 9: Extract features and save to CSV\n",
    "#     output_csv = 'features.csv'\n",
    "#     #extract_features_and_save(images, labels, cnn_model, output_csv, distances, angles)\n",
    "\n",
    "#     # Step 10: Load CSV and prepare for ANN training\n",
    "#     data = pd.read_csv(output_csv)\n",
    "#     X = data.iloc[:, :-1].values  # All columns except the last one (features)\n",
    "#     y = data.iloc[:, -1].values  # Last column (labels)\n",
    "    \n",
    "#     # One-hot encode labels for multi-class classification\n",
    "#     y = tf.keras.utils.to_categorical(y, num_classes=3)\n",
    "    \n",
    "#     # Preprocessing: Standardize features\n",
    "#     scaler = StandardScaler()\n",
    "#     X = scaler.fit_transform(X)\n",
    "    \n",
    "#     # Split into train/test\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "#     print(f\"X_train.shape[1] : {X_train.shape[1]}\")\n",
    "    \n",
    "#     # Step 11: Create and train ANN\n",
    "#     ann_model = create_ann_model(input_dim=X_train.shape[1])\n",
    "#     ann_model.summary()\n",
    "    \n",
    "#     history = ann_model.fit(X_train, y_train, epochs=100, batch_size=16, validation_split=0.1)\n",
    "    \n",
    "#     acc = history.history['accuracy']\n",
    "#     val_acc = history.history['val_accuracy']\n",
    "\n",
    "#     loss = history.history['loss']\n",
    "#     val_loss = history.history['val_loss']\n",
    "\n",
    "#     epochs = range(1, len(acc)+1)\n",
    "\n",
    "#     plt.plot(epochs, acc, label='training accuracy')\n",
    "#     plt.plot(epochs, val_acc, label='validation accuracy')\n",
    "#     plt.legend()\n",
    "#     plt.xlabel('epochs')\n",
    "#     plt.ylabel('accuracy')\n",
    "#     plt.title('Training validation Accuracy')\n",
    "#     plt.show()\n",
    "\n",
    "#     plt.plot(epochs, loss, label='training loss')\n",
    "#     plt.plot(epochs, val_loss, label='validation loss')\n",
    "#     plt.legend()\n",
    "#     plt.xlabel('epochs')\n",
    "#     plt.ylabel('loss')\n",
    "#     plt.title('Training validation Loss')\n",
    "#     plt.show()\n",
    "\n",
    "#     # Step 12: Evaluate the model\n",
    "#     evaluate_model(ann_model, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
