{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa703df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProjectWork\\Basic_Python\\Zero_shot_Learning\\venv_zsl\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# from tensorflow.keras import layers\n",
    "# from tensorflow.keras.preprocessing import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7727d266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define hyperparameters\n",
    "# image_size = (64, 64)\n",
    "# batch_size = 128\n",
    "# latent_dim = 100\n",
    "# num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8db30c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144 files belonging to 18 classes.\n"
     ]
    }
   ],
   "source": [
    "# # Load and preprocess the dataset\n",
    "# def get_dataloader(data_dir, batch_size):\n",
    "#     dataset = image_dataset_from_directory(\n",
    "#         data_dir,\n",
    "#         image_size=image_size,\n",
    "#         batch_size=batch_size,\n",
    "#         label_mode='int'\n",
    "#     )\n",
    "#     dataset = dataset.map(lambda x, y: ((x / 127.5) - 1.0, y))  # Normalize to [-1,1]\n",
    "#     return dataset\n",
    "\n",
    "# # TRAIN_DATA_PATH = os.path.join('..', 'Dataset', 'artefact1', 'train')\n",
    "# # VAL_DATA_PATH = os.path.join('..', 'Dataset', 'artefact1', 'test')\n",
    "# # UNSEEN_DATA_PATH = os.path.join('..', 'Dataset', 'artefact1', 'unseen')\n",
    "\n",
    "# data_dir = os.path.join('..', 'Dataset', 'artefact1', 'test')\n",
    "# dataset = get_dataloader(data_dir, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63327bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the Generator model\n",
    "# def build_generator():\n",
    "#     model = tf.keras.Sequential([\n",
    "#         layers.Dense(8 * 8 * 256, use_bias=False, input_shape=(latent_dim,)),\n",
    "#         layers.BatchNormalization(),\n",
    "#         layers.LeakyReLU(),\n",
    "#         layers.Reshape((8, 8, 256)),\n",
    "#         layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n",
    "#         layers.BatchNormalization(),\n",
    "#         layers.LeakyReLU(),\n",
    "#         layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n",
    "#         layers.BatchNormalization(),\n",
    "#         layers.LeakyReLU(),\n",
    "#         layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', activation='tanh')\n",
    "#     ])\n",
    "#     return model\n",
    "\n",
    "# # Define the Discriminator model\n",
    "# def build_discriminator():\n",
    "#     model = tf.keras.Sequential([\n",
    "#         layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=(64, 64, 3)),\n",
    "#         layers.LeakyReLU(alpha=0.2),\n",
    "#         layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'),\n",
    "#         layers.BatchNormalization(),\n",
    "#         layers.LeakyReLU(alpha=0.2),\n",
    "#         layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'),\n",
    "#         layers.BatchNormalization(),\n",
    "#         layers.LeakyReLU(alpha=0.2),\n",
    "#         layers.Flatten(),\n",
    "#         layers.Dense(1, activation='sigmoid')\n",
    "#     ])\n",
    "#     return model\n",
    "\n",
    "# generator = build_generator()\n",
    "# discriminator = build_discriminator()\n",
    "\n",
    "# discriminator.compile(optimizer=tf.keras.optimizers.Adam(0.0002, 0.5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# gan = tf.keras.Sequential([generator, discriminator])\n",
    "# gan.compile(optimizer=tf.keras.optimizers.Adam(0.0002, 0.5), loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "247a5c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Define optimizers explicitly\n",
    "# generator_optimizer = tf.keras.optimizers.Adam(0.0002, 0.5)\n",
    "# discriminator_optimizer = tf.keras.optimizers.Adam(0.0002, 0.5)\n",
    "\n",
    "# # Training function\n",
    "# def train_gan():\n",
    "#     for epoch in range(num_epochs):\n",
    "#         for real_images, _ in dataset:\n",
    "#             batch_size = tf.shape(real_images)[0]\n",
    "#             noise = tf.random.normal([batch_size, latent_dim])\n",
    "#             fake_images = generator(noise, training=True)\n",
    "#             real_labels = tf.ones((batch_size, 1))\n",
    "#             fake_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "#             # Train Discriminator\n",
    "#             with tf.GradientTape() as tape_d:\n",
    "#                 real_loss = tf.keras.losses.binary_crossentropy(real_labels, discriminator(real_images, training=True))\n",
    "#                 fake_loss = tf.keras.losses.binary_crossentropy(fake_labels, discriminator(fake_images, training=True))\n",
    "#                 loss_d = real_loss + fake_loss\n",
    "#             grads_d = tape_d.gradient(loss_d, discriminator.trainable_variables)\n",
    "#             discriminator_optimizer.apply_gradients(zip(grads_d, discriminator.trainable_variables))\n",
    "\n",
    "#             # Train Generator\n",
    "#             with tf.GradientTape() as tape_g:\n",
    "#                 fake_images = generator(noise, training=True)\n",
    "#                 output = discriminator(fake_images, training=True)\n",
    "#                 loss_g = tf.keras.losses.binary_crossentropy(real_labels, output)\n",
    "#             grads_g = tape_g.gradient(loss_g, generator.trainable_variables)\n",
    "#             generator_optimizer.apply_gradients(zip(grads_g, generator.trainable_variables))\n",
    "\n",
    "#         print(f'Epoch {epoch+1}/{num_epochs}, Loss D: {tf.reduce_mean(loss_d).numpy()}, Loss G: {tf.reduce_mean(loss_g).numpy()}')\n",
    "\n",
    "#     generator.save('generator.h5')\n",
    "#     discriminator.save('discriminator.h5')\n",
    "#     print(\"Models saved!\")\n",
    "\n",
    "\n",
    "# # Function to generate and save images\n",
    "# def generate_images(num_images=10):\n",
    "#     generator = tf.keras.models.load_model('generator.h5')\n",
    "#     noise = tf.random.normal([num_images, latent_dim])\n",
    "#     fake_images = generator(noise, training=False)\n",
    "#     fake_images = (fake_images + 1) / 2.0  # Rescale to [0,1]\n",
    "    \n",
    "#     for i in range(num_images):\n",
    "#         plt.imsave(f'generated_image_{i}.png', fake_images[i].numpy())\n",
    "#     print(\"Images generated and saved!\")\n",
    "\n",
    "# # Function to extract features from images and save to CSV\n",
    "# def extract_features(image_path, label):\n",
    "#     image = tf.keras.preprocessing.image.load_img(image_path, target_size=image_size)\n",
    "#     image = tf.keras.preprocessing.image.img_to_array(image) / 255.0\n",
    "#     mean_intensity = np.mean(image)\n",
    "#     max_intensity = np.max(image)\n",
    "#     min_intensity = np.min(image)\n",
    "#     return [label, mean_intensity, max_intensity, min_intensity]\n",
    "\n",
    "# def save_features_to_csv():\n",
    "#     data = []\n",
    "#     for i in range(10):\n",
    "#         data.append(extract_features(f'generated_image_{i}.png', 'generated'))\n",
    "#     df = pd.DataFrame(data, columns=['Label', 'Mean Intensity', 'Max Intensity', 'Min Intensity'])\n",
    "#     df.to_csv('image_features.csv', index=False)\n",
    "#     print(\"Features saved to CSV!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d83dc505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss D: 0.8203086853027344, Loss G: 3.1351661682128906\n",
      "Epoch 2/50, Loss D: 0.517630934715271, Loss G: 3.7966878414154053\n",
      "Epoch 3/50, Loss D: 0.8214179277420044, Loss G: 2.4206299781799316\n",
      "Epoch 4/50, Loss D: 0.30776822566986084, Loss G: 5.512125015258789\n",
      "Epoch 5/50, Loss D: 1.3505908250808716, Loss G: 1.4603431224822998\n",
      "Epoch 6/50, Loss D: 0.5369666814804077, Loss G: 6.8594865798950195\n",
      "Epoch 7/50, Loss D: 0.6867091655731201, Loss G: 1.7497010231018066\n",
      "Epoch 8/50, Loss D: 0.3038310408592224, Loss G: 6.734214782714844\n",
      "Epoch 9/50, Loss D: 0.675646960735321, Loss G: 1.4458985328674316\n",
      "Epoch 10/50, Loss D: 0.47206929326057434, Loss G: 7.6792778968811035\n",
      "Epoch 11/50, Loss D: 0.4471527934074402, Loss G: 1.668407917022705\n",
      "Epoch 12/50, Loss D: 0.42265719175338745, Loss G: 6.4038286209106445\n",
      "Epoch 13/50, Loss D: 1.0201749801635742, Loss G: 1.2448375225067139\n",
      "Epoch 14/50, Loss D: 0.26579415798187256, Loss G: 8.005762100219727\n",
      "Epoch 15/50, Loss D: 0.33105647563934326, Loss G: 3.047244071960449\n",
      "Epoch 16/50, Loss D: 0.5559408664703369, Loss G: 3.6296238899230957\n",
      "Epoch 17/50, Loss D: 0.32161271572113037, Loss G: 3.4115238189697266\n",
      "Epoch 18/50, Loss D: 0.23515227437019348, Loss G: 4.0277533531188965\n",
      "Epoch 19/50, Loss D: 0.2220022976398468, Loss G: 3.435300588607788\n",
      "Epoch 20/50, Loss D: 0.1538664847612381, Loss G: 4.586907386779785\n",
      "Epoch 21/50, Loss D: 0.27134329080581665, Loss G: 3.2979650497436523\n",
      "Epoch 22/50, Loss D: 0.11638253927230835, Loss G: 5.317808628082275\n",
      "Epoch 23/50, Loss D: 0.21408246457576752, Loss G: 3.1010689735412598\n",
      "Epoch 24/50, Loss D: 0.22811190783977509, Loss G: 5.479278564453125\n",
      "Epoch 25/50, Loss D: 0.11910628527402878, Loss G: 3.116921901702881\n",
      "Epoch 26/50, Loss D: 0.09645694494247437, Loss G: 6.372461318969727\n",
      "Epoch 27/50, Loss D: 0.2081298977136612, Loss G: 2.506070137023926\n",
      "Epoch 28/50, Loss D: 0.17947925627231598, Loss G: 8.431440353393555\n",
      "Epoch 29/50, Loss D: 0.4202900230884552, Loss G: 1.1913155317306519\n",
      "Epoch 30/50, Loss D: 0.9939475655555725, Loss G: 13.16537857055664\n",
      "Epoch 31/50, Loss D: 0.21196424961090088, Loss G: 2.876096725463867\n",
      "Epoch 32/50, Loss D: 0.956652045249939, Loss G: 5.853677272796631\n",
      "Epoch 33/50, Loss D: 0.8667387366294861, Loss G: 0.7852873206138611\n",
      "Epoch 34/50, Loss D: 1.782534122467041, Loss G: 7.588776111602783\n",
      "Epoch 35/50, Loss D: 0.9906763434410095, Loss G: 0.802239179611206\n",
      "Epoch 36/50, Loss D: 1.0528013706207275, Loss G: 3.9064974784851074\n",
      "Epoch 37/50, Loss D: 0.9976813197135925, Loss G: 0.8760882616043091\n",
      "Epoch 38/50, Loss D: 1.0846083164215088, Loss G: 2.9516656398773193\n",
      "Epoch 39/50, Loss D: 1.1062088012695312, Loss G: 0.7037498950958252\n",
      "Epoch 40/50, Loss D: 1.5772535800933838, Loss G: 2.906643867492676\n",
      "Epoch 41/50, Loss D: 0.9429084062576294, Loss G: 0.6903336048126221\n",
      "Epoch 42/50, Loss D: 1.0994590520858765, Loss G: 3.3583455085754395\n",
      "Epoch 43/50, Loss D: 1.1306731700897217, Loss G: 0.6743346452713013\n",
      "Epoch 44/50, Loss D: 0.9403153657913208, Loss G: 2.4828009605407715\n",
      "Epoch 45/50, Loss D: 1.0628727674484253, Loss G: 0.6613157987594604\n",
      "Epoch 46/50, Loss D: 0.7219286561012268, Loss G: 2.7860960960388184\n",
      "Epoch 47/50, Loss D: 1.2124903202056885, Loss G: 0.6123126745223999\n",
      "Epoch 48/50, Loss D: 1.1007766723632812, Loss G: 3.1250576972961426\n",
      "Epoch 49/50, Loss D: 0.9174467325210571, Loss G: 0.7896489500999451\n",
      "Epoch 50/50, Loss D: 0.6583430767059326, Loss G: 3.183316707611084\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Models saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProjectWork\\Basic_Python\\Zero_shot_Learning\\venv_zsl\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# train_gan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54a42d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Images generated and saved!\n"
     ]
    }
   ],
   "source": [
    "# generate_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7d0e58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff0e0ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd4878c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c23a8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# from tensorflow.keras import layers, Model\n",
    "# from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "# # Define hyperparameters\n",
    "# image_size = (64, 64)\n",
    "# batch_size = 128\n",
    "# latent_dim = 100\n",
    "# num_epochs = 50\n",
    "\n",
    "# # Load and preprocess the dataset\n",
    "# def get_dataloader(data_dir, batch_size):\n",
    "#     dataset = image_dataset_from_directory(\n",
    "#         data_dir,\n",
    "#         image_size=image_size,\n",
    "#         batch_size=batch_size,\n",
    "#         label_mode='int'\n",
    "#     )\n",
    "#     dataset = dataset.map(lambda x, y: ((x / 127.5) - 1.0, y))  # Normalize to [-1,1]\n",
    "#     return dataset\n",
    "\n",
    "# data_dir = \"path_to_AWA2_dataset\"\n",
    "# dataset = get_dataloader(data_dir, batch_size)\n",
    "\n",
    "# # Define the Generator model\n",
    "# def build_generator():\n",
    "#     model = tf.keras.Sequential([\n",
    "#         layers.Dense(8 * 8 * 256, use_bias=False, input_shape=(latent_dim,)),\n",
    "#         layers.BatchNormalization(),\n",
    "#         layers.LeakyReLU(),\n",
    "#         layers.Reshape((8, 8, 256)),\n",
    "#         layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n",
    "#         layers.BatchNormalization(),\n",
    "#         layers.LeakyReLU(),\n",
    "#         layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n",
    "#         layers.BatchNormalization(),\n",
    "#         layers.LeakyReLU(),\n",
    "#         layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', activation='tanh')\n",
    "#     ])\n",
    "#     return model\n",
    "\n",
    "# # Define the Discriminator model\n",
    "# def build_discriminator():\n",
    "#     model = tf.keras.Sequential([\n",
    "#         layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=(64, 64, 3)),\n",
    "#         layers.LeakyReLU(alpha=0.2),\n",
    "#         layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'),\n",
    "#         layers.BatchNormalization(),\n",
    "#         layers.LeakyReLU(alpha=0.2),\n",
    "#         layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'),\n",
    "#         layers.BatchNormalization(),\n",
    "#         layers.LeakyReLU(alpha=0.2),\n",
    "#         layers.Flatten(),\n",
    "#         layers.Dense(1, activation='sigmoid')\n",
    "#     ])\n",
    "#     return model\n",
    "\n",
    "# generator = build_generator()\n",
    "# discriminator = build_discriminator()\n",
    "\n",
    "# generator_optimizer = tf.keras.optimizers.Adam(0.0002, 0.5)\n",
    "# discriminator_optimizer = tf.keras.optimizers.Adam(0.0002, 0.5)\n",
    "\n",
    "# # Training function\n",
    "# def train_gan():\n",
    "#     for epoch in range(num_epochs):\n",
    "#         for real_images, _ in dataset:\n",
    "#             batch_size = tf.shape(real_images)[0]\n",
    "#             noise = tf.random.normal([batch_size, latent_dim])\n",
    "#             fake_images = generator(noise, training=True)\n",
    "#             real_labels = tf.ones((batch_size, 1))\n",
    "#             fake_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "#             # Train Discriminator\n",
    "#             with tf.GradientTape() as tape_d:\n",
    "#                 real_loss = tf.keras.losses.binary_crossentropy(real_labels, discriminator(real_images, training=True))\n",
    "#                 fake_loss = tf.keras.losses.binary_crossentropy(fake_labels, discriminator(fake_images, training=True))\n",
    "#                 loss_d = real_loss + fake_loss\n",
    "#             grads_d = tape_d.gradient(loss_d, discriminator.trainable_variables)\n",
    "#             discriminator_optimizer.apply_gradients(zip(grads_d, discriminator.trainable_variables))\n",
    "\n",
    "#             # Train Generator\n",
    "#             with tf.GradientTape() as tape_g:\n",
    "#                 fake_images = generator(noise, training=True)\n",
    "#                 output = discriminator(fake_images, training=True)\n",
    "#                 loss_g = tf.keras.losses.binary_crossentropy(real_labels, output)\n",
    "#             grads_g = tape_g.gradient(loss_g, generator.trainable_variables)\n",
    "#             generator_optimizer.apply_gradients(zip(grads_g, generator.trainable_variables))\n",
    "\n",
    "#         print(f'Epoch {epoch+1}/{num_epochs}, Loss D: {tf.reduce_mean(loss_d).numpy()}, Loss G: {tf.reduce_mean(loss_g).numpy()}')\n",
    "    \n",
    "#     generator.save('generator.h5')\n",
    "#     discriminator.save('discriminator.h5')\n",
    "#     print(\"Models saved!\")\n",
    "\n",
    "# # Function to generate and save images\n",
    "# def generate_images(num_images=10):\n",
    "#     generator = tf.keras.models.load_model('generator.h5')\n",
    "#     noise = tf.random.normal([num_images, latent_dim])\n",
    "#     fake_images = generator(noise, training=False)\n",
    "#     fake_images = (fake_images + 1) / 2.0  # Rescale to [0,1]\n",
    "    \n",
    "#     for i in range(num_images):\n",
    "#         plt.imsave(f'generated_image_{i}.png', fake_images[i].numpy())\n",
    "#     print(\"Images generated and saved!\")\n",
    "\n",
    "# # Load pre-trained GAN model (Ensure you have trained it)\n",
    "# generator = tf.keras.models.load_model(\"generator.h5\")  \n",
    "# discriminator = tf.keras.models.load_model(\"discriminator.h5\")\n",
    "\n",
    "# # Modify discriminator to output features from an intermediate layer\n",
    "# feature_extractor = Model(inputs=discriminator.input, outputs=discriminator.layers[-2].output)  # Get second last layer\n",
    "\n",
    "# # Function to extract features and save to CSV\n",
    "# def extract_features(image_path, label):\n",
    "#     image = tf.keras.preprocessing.image.load_img(image_path, target_size=image_size)\n",
    "#     image = tf.keras.preprocessing.image.img_to_array(image) / 255.0\n",
    "#     image = np.expand_dims(image, axis=0)\n",
    "#     features = feature_extractor.predict(image)[0]\n",
    "#     return [label] + features.tolist()\n",
    "\n",
    "# def save_features_to_csv():\n",
    "#     data = []\n",
    "#     for i in range(10):\n",
    "#         data.append(extract_features(f'generated_image_{i}.png', 'generated'))\n",
    "#     df = pd.DataFrame(data)\n",
    "#     df.to_csv('image_features.csv', index=False)\n",
    "#     print(\"Features saved to CSV!\")\n",
    "\n",
    "# # Run the training process\n",
    "# train_gan()\n",
    "# generate_images()\n",
    "# save_features_to_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e070e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5846bab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac23de14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2720587f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 720 files belonging to 18 classes.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Epoch 1/50, Loss D: 1.2528941631317139, Loss G: 1.632101058959961\n",
      "Epoch 2/50, Loss D: 1.1220314502716064, Loss G: 1.3427681922912598\n",
      "Epoch 3/50, Loss D: 1.5939379930496216, Loss G: 1.0195574760437012\n",
      "Epoch 4/50, Loss D: 1.6914100646972656, Loss G: 1.5572463274002075\n",
      "Epoch 5/50, Loss D: 0.7529237270355225, Loss G: 1.6565356254577637\n",
      "Epoch 6/50, Loss D: 0.9616453647613525, Loss G: 1.4820258617401123\n",
      "Epoch 7/50, Loss D: 1.340908408164978, Loss G: 1.1183966398239136\n",
      "Epoch 8/50, Loss D: 0.9590020179748535, Loss G: 1.4952281713485718\n",
      "Epoch 9/50, Loss D: 0.9446175694465637, Loss G: 1.6298644542694092\n",
      "Epoch 10/50, Loss D: 0.7151741981506348, Loss G: 2.5885157585144043\n",
      "Epoch 11/50, Loss D: 0.7326241135597229, Loss G: 2.2383875846862793\n",
      "Epoch 12/50, Loss D: 0.9893332719802856, Loss G: 2.7690303325653076\n",
      "Epoch 13/50, Loss D: 0.8605053424835205, Loss G: 2.715982437133789\n",
      "Epoch 14/50, Loss D: 1.4958794116973877, Loss G: 1.6722450256347656\n",
      "Epoch 15/50, Loss D: 1.386782169342041, Loss G: 1.3334139585494995\n",
      "Epoch 16/50, Loss D: 1.329404592514038, Loss G: 1.1696743965148926\n",
      "Epoch 17/50, Loss D: 0.6536022424697876, Loss G: 2.284886360168457\n",
      "Epoch 18/50, Loss D: 0.7234559655189514, Loss G: 1.9768493175506592\n",
      "Epoch 19/50, Loss D: 1.1895378828048706, Loss G: 3.8095579147338867\n",
      "Epoch 20/50, Loss D: 0.7756559252738953, Loss G: 2.1661465167999268\n",
      "Epoch 21/50, Loss D: 0.5711084604263306, Loss G: 2.005854368209839\n",
      "Epoch 22/50, Loss D: 0.8718671798706055, Loss G: 1.7754169702529907\n",
      "Epoch 23/50, Loss D: 0.5327781438827515, Loss G: 2.468035936355591\n",
      "Epoch 24/50, Loss D: 1.5557852983474731, Loss G: 1.565346598625183\n",
      "Epoch 25/50, Loss D: 0.7106120586395264, Loss G: 1.8211071491241455\n",
      "Epoch 26/50, Loss D: 0.6451172828674316, Loss G: 2.1002631187438965\n",
      "Epoch 27/50, Loss D: 0.934633731842041, Loss G: 2.0570056438446045\n",
      "Epoch 28/50, Loss D: 1.3431479930877686, Loss G: 1.9597549438476562\n",
      "Epoch 29/50, Loss D: 1.5122019052505493, Loss G: 1.306511402130127\n",
      "Epoch 30/50, Loss D: 1.0367143154144287, Loss G: 1.7376124858856201\n",
      "Epoch 31/50, Loss D: 1.5857765674591064, Loss G: 1.237307071685791\n",
      "Epoch 32/50, Loss D: 1.0088969469070435, Loss G: 1.0041179656982422\n",
      "Epoch 33/50, Loss D: 1.1202428340911865, Loss G: 1.4774876832962036\n",
      "Epoch 34/50, Loss D: 1.2065151929855347, Loss G: 1.8676557540893555\n",
      "Epoch 35/50, Loss D: 0.9174747467041016, Loss G: 2.425790309906006\n",
      "Epoch 36/50, Loss D: 0.6847986578941345, Loss G: 2.736988067626953\n",
      "Epoch 37/50, Loss D: 0.5534856915473938, Loss G: 1.993646264076233\n",
      "Epoch 38/50, Loss D: 1.050174593925476, Loss G: 1.7724394798278809\n",
      "Epoch 39/50, Loss D: 0.7040654420852661, Loss G: 2.348008155822754\n",
      "Epoch 40/50, Loss D: 1.0031789541244507, Loss G: 1.346717357635498\n",
      "Epoch 41/50, Loss D: 0.7194338440895081, Loss G: 1.9934616088867188\n",
      "Epoch 42/50, Loss D: 0.7829108834266663, Loss G: 1.9543837308883667\n",
      "Epoch 43/50, Loss D: 0.2887043058872223, Loss G: 2.579310894012451\n",
      "Epoch 44/50, Loss D: 0.623326301574707, Loss G: 1.902567982673645\n",
      "Epoch 45/50, Loss D: 0.533523440361023, Loss G: 1.7898941040039062\n",
      "Epoch 46/50, Loss D: 1.047207236289978, Loss G: 1.7604854106903076\n",
      "Epoch 47/50, Loss D: 0.6959512829780579, Loss G: 2.2111730575561523\n",
      "Epoch 48/50, Loss D: 0.6000529527664185, Loss G: 2.2871320247650146\n",
      "Epoch 49/50, Loss D: 0.595879852771759, Loss G: 2.700885057449341\n",
      "Epoch 50/50, Loss D: 1.3246235847473145, Loss G: 2.4710700511932373\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Models saved!\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Images generated and saved!\n",
      "1/1 [==============================] - 0s 234ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 203ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Features saved to CSV!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# Define hyperparameters\n",
    "image_size = (64, 64)\n",
    "batch_size = 32 # 128\n",
    "latent_dim = 100\n",
    "num_epochs = 50\n",
    "\n",
    "data_dir = os.path.join('..', 'Dataset', 'artefact1', 'train')\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def get_dataloader(data_dir, batch_size):\n",
    "    dataset = image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        label_mode='int'\n",
    "    )\n",
    "    dataset = dataset.map(lambda x, y: ((x / 127.5) - 1.0, y))  # Normalize to [-1,1]\n",
    "    return dataset\n",
    "\n",
    "dataset = get_dataloader(data_dir, batch_size)\n",
    "\n",
    "# Define the Generator model\n",
    "def build_generator():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(8 * 8 * 256, use_bias=False, input_shape=(latent_dim,)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Reshape((8, 8, 256)),\n",
    "        layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', activation='tanh')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define the Discriminator model\n",
    "def build_discriminator():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=(64, 64, 3)),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(0.0002, 0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(0.0002, 0.5)\n",
    "\n",
    "# Training function\n",
    "def train_gan():\n",
    "    for epoch in range(num_epochs):\n",
    "        for real_images, _ in dataset:\n",
    "            batch_size = tf.shape(real_images)[0]\n",
    "            noise = tf.random.normal([batch_size, latent_dim])\n",
    "            fake_images = generator(noise, training=True)\n",
    "            real_labels = tf.ones((batch_size, 1))\n",
    "            fake_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "            # Train Discriminator\n",
    "            with tf.GradientTape() as tape_d:\n",
    "                real_loss = tf.keras.losses.binary_crossentropy(real_labels, discriminator(real_images, training=True))\n",
    "                fake_loss = tf.keras.losses.binary_crossentropy(fake_labels, discriminator(fake_images, training=True))\n",
    "                loss_d = real_loss + fake_loss\n",
    "            grads_d = tape_d.gradient(loss_d, discriminator.trainable_variables)\n",
    "            discriminator_optimizer.apply_gradients(zip(grads_d, discriminator.trainable_variables))\n",
    "\n",
    "            # Train Generator\n",
    "            with tf.GradientTape() as tape_g:\n",
    "                fake_images = generator(noise, training=True)\n",
    "                output = discriminator(fake_images, training=True)\n",
    "                loss_g = tf.keras.losses.binary_crossentropy(real_labels, output)\n",
    "            grads_g = tape_g.gradient(loss_g, generator.trainable_variables)\n",
    "            generator_optimizer.apply_gradients(zip(grads_g, generator.trainable_variables))\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss D: {tf.reduce_mean(loss_d).numpy()}, Loss G: {tf.reduce_mean(loss_g).numpy()}')\n",
    "    \n",
    "    generator.save('generator.h5')\n",
    "    discriminator.save('discriminator.h5')\n",
    "    print(\"Models saved!\")\n",
    "\n",
    "# Function to generate and save images\n",
    "def generate_images(num_images=10):\n",
    "    generator = tf.keras.models.load_model('generator.h5')\n",
    "    noise = tf.random.normal([num_images, latent_dim])\n",
    "    fake_images = generator(noise, training=False)\n",
    "    fake_images = (fake_images + 1) / 2.0  # Rescale to [0,1]\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        plt.imsave(f'generated_image_{i}.png', fake_images[i].numpy())\n",
    "    print(\"Images generated and saved!\")\n",
    "\n",
    "# Load pre-trained GAN model\n",
    "generator = tf.keras.models.load_model(\"generator.h5\")  \n",
    "discriminator = tf.keras.models.load_model(\"discriminator.h5\")\n",
    "\n",
    "# Modify discriminator to output features from an intermediate layer\n",
    "feature_extractor = Model(inputs=discriminator.input, outputs=discriminator.layers[-2].output)  # Get second last layer\n",
    "\n",
    "# Function to extract features and save to CSV\n",
    "def extract_features(image_path, label):\n",
    "    image = load_img(image_path, target_size=image_size)\n",
    "    image = img_to_array(image) / 255.0\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    features = feature_extractor.predict(image)[0]\n",
    "    return [label] + features.tolist()\n",
    "\n",
    "def save_features_to_csv():\n",
    "    data = []\n",
    "    for class_name in os.listdir(data_dir):\n",
    "        class_path = os.path.join(data_dir, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            for image_name in os.listdir(class_path):\n",
    "                image_path = os.path.join(class_path, image_name)\n",
    "                data.append(extract_features(image_path, class_name))\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv('image_features.csv', index=False)\n",
    "    print(\"Features saved to CSV!\")\n",
    "\n",
    "# Run the training process\n",
    "train_gan()\n",
    "generate_images()\n",
    "save_features_to_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16fbbd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae906a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8357426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# Define hyperparameters\n",
    "image_size = (224, 224)\n",
    "batch_size = 128\n",
    "latent_dim = 100\n",
    "num_epochs = 50\n",
    "\n",
    "data_dir = \"path_to_AWA2_dataset\"\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def get_dataloader(data_dir, batch_size):\n",
    "    dataset = image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        label_mode='int'\n",
    "    )\n",
    "    dataset = dataset.map(lambda x, y: ((x / 127.5) - 1.0, y))  # Normalize to [-1,1]\n",
    "    return dataset\n",
    "\n",
    "dataset = get_dataloader(data_dir, batch_size)\n",
    "\n",
    "# Define the Generator model\n",
    "def build_generator():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(14 * 14 * 256, use_bias=False, input_shape=(latent_dim,)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Reshape((14, 14, 256)),\n",
    "        layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', activation='tanh')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define the Discriminator model\n",
    "def build_discriminator():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=(224, 224, 3)),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(0.0002, 0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(0.0002, 0.5)\n",
    "\n",
    "# Training function\n",
    "def train_gan():\n",
    "    for epoch in range(num_epochs):\n",
    "        for real_images, _ in dataset:\n",
    "            batch_size = tf.shape(real_images)[0]\n",
    "            noise = tf.random.normal([batch_size, latent_dim])\n",
    "            fake_images = generator(noise, training=True)\n",
    "            real_labels = tf.ones((batch_size, 1))\n",
    "            fake_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "            # Train Discriminator\n",
    "            with tf.GradientTape() as tape_d:\n",
    "                real_loss = tf.keras.losses.binary_crossentropy(real_labels, discriminator(real_images, training=True))\n",
    "                fake_loss = tf.keras.losses.binary_crossentropy(fake_labels, discriminator(fake_images, training=True))\n",
    "                loss_d = real_loss + fake_loss\n",
    "            grads_d = tape_d.gradient(loss_d, discriminator.trainable_variables)\n",
    "            discriminator_optimizer.apply_gradients(zip(grads_d, discriminator.trainable_variables))\n",
    "\n",
    "            # Train Generator\n",
    "            with tf.GradientTape() as tape_g:\n",
    "                fake_images = generator(noise, training=True)\n",
    "                output = discriminator(fake_images, training=True)\n",
    "                loss_g = tf.keras.losses.binary_crossentropy(real_labels, output)\n",
    "            grads_g = tape_g.gradient(loss_g, generator.trainable_variables)\n",
    "            generator_optimizer.apply_gradients(zip(grads_g, generator.trainable_variables))\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss D: {tf.reduce_mean(loss_d).numpy()}, Loss G: {tf.reduce_mean(loss_g).numpy()}')\n",
    "    \n",
    "    generator.save('generator.h5')\n",
    "    discriminator.save('discriminator.h5')\n",
    "    print(\"Models saved!\")\n",
    "\n",
    "# Function to generate and save images\n",
    "def generate_images(num_images=10):\n",
    "    generator = tf.keras.models.load_model('generator.h5')\n",
    "    noise = tf.random.normal([num_images, latent_dim])\n",
    "    fake_images = generator(noise, training=False)\n",
    "    fake_images = (fake_images + 1) / 2.0  # Rescale to [0,1]\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        plt.imsave(f'generated_image_{i}.png', fake_images[i].numpy())\n",
    "    print(\"Images generated and saved!\")\n",
    "\n",
    "# Load pre-trained GAN model\n",
    "generator = tf.keras.models.load_model(\"generator.h5\")  \n",
    "discriminator = tf.keras.models.load_model(\"discriminator.h5\")\n",
    "\n",
    "# Modify discriminator to output features from an intermediate layer\n",
    "feature_extractor = Model(inputs=discriminator.input, outputs=discriminator.layers[-2].output)  # Get second last layer\n",
    "\n",
    "# Function to extract features and save to CSV\n",
    "def extract_features(image_path, label):\n",
    "    image = load_img(image_path, target_size=image_size)\n",
    "    image = img_to_array(image) / 255.0\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    features = feature_extractor.predict(image)[0]\n",
    "    return [label] + features.tolist()\n",
    "\n",
    "def save_features_to_csv():\n",
    "    data = []\n",
    "    for class_name in os.listdir(data_dir):\n",
    "        class_path = os.path.join(data_dir, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            for image_name in os.listdir(class_path):\n",
    "                image_path = os.path.join(class_path, image_name)\n",
    "                data.append(extract_features(image_path, class_name))\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv('image_features.csv', index=False)\n",
    "    print(\"Features saved to CSV!\")\n",
    "\n",
    "# Run the training process\n",
    "train_gan()\n",
    "generate_images()\n",
    "save_features_to_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2bd62f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef71b740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfde80f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8b6fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e590ead2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c034f20a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92705eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9871b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64203774",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
