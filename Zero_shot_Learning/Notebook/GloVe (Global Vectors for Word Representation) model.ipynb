{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05b21874",
   "metadata": {},
   "source": [
    "# GloVe (Global Vectors for Word Representation) model\n",
    "\n",
    "glove.6B.300d.txt is a pre-trained word embedding file from the GloVe (Global Vectors for Word Representation) model, developed by researchers at Stanford. Here's what it is and what it contains:\n",
    "\n",
    "#### Key Features:\n",
    "\n",
    "1. Pre-trained Word Embeddings:\n",
    "    - GloVe embeddings are pre-trained on a large corpus of text to capture semantic meaning, word similarity, and relationships.\n",
    "    - The 6B in the filename refers to the corpus used for training, specifically 6 billion tokens (words) from a dataset including Wikipedia and Gigaword.\n",
    "    \n",
    "2. Vector Dimensionality:\n",
    "    - The 300d indicates the dimensionality of the word vectors. Each word is represented as a 300-dimensional numerical vector.\n",
    "    \n",
    "3. Format:\n",
    "    - The file is in plain text, with each line containing:\n",
    "    \n",
    "        `\n",
    "        word v1 v2 v3 ... v300\n",
    "        `\n",
    "    \n",
    "    Where word is the vocabulary term, and v1 to v300 are the 300-dimensional vector components.\n",
    "\n",
    "4. Vocabulary:\n",
    "    - This particular file includes 400,000 unique words or tokens.\n",
    "    \n",
    "5. Applications:\n",
    "    - Natural Language Processing (NLP) tasks such as text classification, sentiment analysis, question answering, machine translation, and more.\n",
    "    - The embeddings are used as input to machine learning models to represent textual data numerically.\n",
    "    \n",
    "6. Advantages:\n",
    "    - Captures both semantic (e.g., king-queen, man-woman) and syntactic (e.g., walking-walked, swimming-swam) relationships.\n",
    "    - Useful for downstream tasks without requiring the training of embeddings from scratch.\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "951be950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The GloVe official website : https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "# To install\n",
    "# !pip install gensim "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbd0948",
   "metadata": {},
   "source": [
    "## To create a GloVe-like file (glove.6B.300d.txt) with embeddings for your own words\n",
    "\n",
    "To create a GloVe-like file (glove.6B.300d.txt) with embeddings for your own words, you can use a pre-trained model (such as GloVe, Word2Vec, or FastText) to extract embeddings for your specific words.\n",
    "\n",
    "Creating a pre-trained word embedding file involves training a Word2Vec model or downloading a pre-trained one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a0f518",
   "metadata": {},
   "source": [
    "### Option 1: Download the Pre-Trained Word2Vec File\n",
    "\n",
    "The Google News Word2Vec embeddings are widely used and publicly available.\n",
    "\n",
    "1. Download the Pre-Trained File:\n",
    "    - Visit the official GoogleNews-vectors repository or use the hosted version from other reliable sources like Kaggle. \n",
    "    - Direct download link: GoogleNews-vectors-negative300.bin.gz\n",
    "    - [kaggle](https://www.kaggle.com/datasets/leadbest/googlenewsvectorsnegative300?select=GoogleNews-vectors-negative300.bin.gz)\n",
    "    - [github](https://github.com/mmihaltz/word2vec-GoogleNews-vectors/blob/master/GoogleNews-vectors-negative300.bin.gz)\n",
    "    - [drive](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/view?resourcekey=0-wjGZdNAUop6WykTtMip30g)\n",
    "    \n",
    "2. Save the File:\n",
    "    - Save the file to a local directory on your machine.\n",
    "\n",
    "3. Use the File:\n",
    "    - Load it into Python using the gensim library as shown in the earlier example:\n",
    "        \n",
    "        ```Python\n",
    "        from gensim.models import KeyedVectors\n",
    "        model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "        ```\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb07bf10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: KeyedVectors<vector_size=300, 3000000 keys>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "file_path = os.path.join('GoogleNews_vectors_negative300', '1', 'GoogleNews-vectors-negative300.bin.gz')\n",
    "model = KeyedVectors.load_word2vec_format(file_path, binary=True)\n",
    "print(f\"model: {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb1fed8",
   "metadata": {},
   "source": [
    "### Option 2: Train Your Own Word2Vec Model\n",
    "\n",
    "If you prefer to train your own Word2Vec embeddings for a custom dataset:\n",
    "\n",
    "#### Steps to Train Word2Vec\n",
    "\n",
    "1. Prepare a Text Corpus:\n",
    "    - Collect a large corpus of text data related to your domain. Save it as a .txt file.\n",
    "\n",
    "2. Install Gensim:\n",
    "    \n",
    "    `\n",
    "    pip install gensim\n",
    "    `\n",
    "\n",
    "3. Train the Word2Vec Model: Use the gensim library to train a Word2Vec model.\n",
    "\n",
    "    ```Python\n",
    "    from gensim.models import Word2Vec\n",
    "\n",
    "    # Load your text corpus\n",
    "    corpus_file = 'your_text_corpus.txt'  # Replace with your text corpus file\n",
    "    with open(corpus_file, 'r', encoding='utf-8') as f:\n",
    "        sentences = [line.strip().split() for line in f]  # Tokenize sentences\n",
    "\n",
    "    # Train Word2Vec model\n",
    "    model = Word2Vec(\n",
    "        sentences,\n",
    "        vector_size=300,  # Number of dimensions for the embeddings\n",
    "        window=5,         # Context window size\n",
    "        min_count=5,      # Minimum word frequency to include in the vocabulary\n",
    "        workers=4         # Number of threads\n",
    "    )\n",
    "\n",
    "    # Save the model in binary format\n",
    "    model.wv.save_word2vec_format('custom_word2vec.bin', binary=True)\n",
    "    print(\"Word2Vec model saved as 'custom_word2vec.bin'\")\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d095d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load your text corpus\n",
    "corpus_file = 'your_text_corpus.txt'  # Replace with your text corpus file\n",
    "with open(corpus_file, 'r', encoding='utf-8') as f:\n",
    "    sentences = [line.strip().split() for line in f]  # Tokenize sentences\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=300,  # Number of dimensions for the embeddings\n",
    "    window=5,         # Context window size\n",
    "    min_count=5,      # Minimum word frequency to include in the vocabulary\n",
    "    workers=4         # Number of threads\n",
    ")\n",
    "\n",
    "# Save the model in binary format\n",
    "model.wv.save_word2vec_format('custom_word2vec.bin', binary=True)\n",
    "print(\"Word2Vec model saved as 'custom_word2vec.bin'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
