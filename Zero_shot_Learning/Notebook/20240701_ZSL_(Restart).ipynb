{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57303226",
   "metadata": {},
   "source": [
    "# Gensim Fasttext\n",
    "\n",
    "Gensim provides a convenient implementation of FastText, which can be used to train word vectors on a custom corpus or to use pre-trained models for various tasks such as finding similar words and computing similarity scores between words.\n",
    "\n",
    "### Installing Gensim\n",
    "\n",
    "> pip install gensim\n",
    "\n",
    "### Using Gensim FastText\n",
    "\n",
    "Here are the key steps to use Gensim FastText:\n",
    "\n",
    "1. Loading a Pre-trained Model\n",
    "2. Training a FastText Model on a Custom Corpus\n",
    "3. Finding Similar Words\n",
    "4. Computing Similarity Scores Between Words\n",
    "\n",
    "### Links\n",
    "\n",
    "[Migrating-from-Gensim-3.x-to-4](https://github.com/piskvorky/gensim/wiki/Migrating-from-Gensim-3.x-to-4)\n",
    "\n",
    "[cub-200-2011_paper](https://paperswithcode.com/dataset/cub-200-2011)\n",
    "\n",
    "[cub-200-2011_dataset](https://www.kaggle.com/datasets/wenewone/cub2002011?resource=download)\n",
    "\n",
    "[Microsoft C++ Build Tools](https://visualstudio.microsoft.com/visual-cpp-build-tools/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f04785f",
   "metadata": {},
   "source": [
    "### 1. Loading a Pre-trained Model\n",
    "\n",
    "Gensim provides a way to load pre-trained FastText models. For example, you can load the pre-trained FastText model provided by Facebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c70a6dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyedVectors<vector_size=300, 999999 keys>\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Load pre-trained FastText model\n",
    "model = api.load('fasttext-wiki-news-subwords-300')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef73e23",
   "metadata": {},
   "source": [
    "### 2. Training a FastText Model on a Custom Corpus\n",
    "\n",
    "You can also train a FastText model on your custom corpus. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e3e6e74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences : [['african_buffalo'], ['alligator'], ['amphibian'], ['amur_leopard'], ['ants'], ['bear'], ['bird'], ['blue_whale'], ['bobcat'], ['cat'], ['chimp'], ['chimpanzee'], ['cow'], ['dog'], ['dolphin'], [], ['eagle'], ['elephant'], ['fish'], ['frog'], ['giant'], ['giant_panda'], ['goat'], ['gorilla'], ['hen'], ['horse'], ['killer_whale'], ['lion'], ['lizard'], ['monkey'], ['mouse'], ['orangutan'], ['ostrich'], ['ox'], ['panda'], ['polar_bear'], ['rabbit'], ['rat'], ['rhino'], ['rhinoceros'], ['rhinoceroses'], ['seal'], ['sealskin'], ['siamese_cat'], ['skunk'], ['spider_monkey'], ['squirrel'], ['tiger'], ['turtle'], ['walrus'], ['whale'], ['bird'], ['fish'], ['lion'], ['tiger'], ['bull']] \n",
      "\n",
      "model : FastText<vocab=51, vector_size=300, alpha=0.025> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Example sentences\n",
    "# sentences = [\n",
    "#     \"Cats and dogs are both popular household pets, yet cats are more independent and often prefer solitude. They share some hunting instincts with their larger feline cousins like lions and tigers.\",\n",
    "#     \"Dogs and cats are common pets, but dogs are known for their loyalty and tendency to form strong bonds with humans. Unlike solitary big cats, dogs are social animals that thrive in packs.\",\n",
    "#     \"Horses, like elephants, have been domesticated to assist humans in various tasks. However, horses are known for their speed and agility, whereas elephants are prized for their strength and intelligence.\",\n",
    "#     \"Lions and tigers are both apex predators, but lions are social animals living in prides. In contrast, tigers are solitary creatures, only coming together during mating or to raise cubs.\",\n",
    "#     \"Tigers share their powerful physique and hunting prowess with lions. Unlike the social lions, tigers are mostly solitary, showcasing a stark behavioral difference between the two big cats.\",\n",
    "#     \"Elephants, similar to horses, have been used by humans for labor due to their strength. Elephants, however, are highly intelligent with complex social structures, unlike the more individually task-oriented horses.\",\n",
    "# ]\n",
    "sentences = [\n",
    "    'african_buffalo', 'alligator', 'amphibian', 'amur_leopard', \n",
    "    'ants', 'bear', 'bird', 'blue_whale', 'bobcat', 'cat', 'chimp', \n",
    "    'chimpanzee', 'cow', 'dog', 'dolphin', 'domestic_water_buffalo', \n",
    "    'eagle', 'elephant', 'fish', 'frog', 'giant', 'giant_panda', 'goat', \n",
    "    'gorilla', 'hen', 'horse', 'killer_whale', 'lion', 'lizard', 'monkey', \n",
    "    'mouse', 'orangutan', 'ostrich', 'ox', 'panda', 'polar_bear', 'rabbit', \n",
    "    'rat', 'rhino', 'rhinoceros', 'rhinoceroses', 'seal', 'sealskin', \n",
    "    'siamese_cat', 'skunk', 'spider_monkey', 'squirrel', 'tiger', 'turtle', \n",
    "    'walrus', 'whale', 'bird', 'fish', 'lion', 'tiger', 'bull'\n",
    "]\n",
    "\n",
    "# Preprocess sentences\n",
    "sentences = [simple_preprocess(sentence) for sentence in sentences]\n",
    "\n",
    "print(f\"sentences : {sentences} \\n\")\n",
    "\n",
    "# Train FastText model\n",
    "model = FastText(sentences, vector_size=300, window=5, min_count=1, epochs=10000)\n",
    "\n",
    "print(f\"model : {model} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d89e93e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<gensim.models.fasttext.FastTextKeyedVectors at 0x17e562e5600>, 51, 300)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv, len(model.wv), len(model.wv[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98fc12d",
   "metadata": {},
   "source": [
    "### 3. Finding Similar Words\n",
    "\n",
    "Once you have a trained or pre-trained model, you can find similar words using the `most_similar` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86a5230d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bobcat', 0.2737165093421936), ('goat', 0.15198037028312683), ('bird', 0.12732714414596558), ('ostrich', 0.09741493314504623), ('seal', 0.09225074201822281), ('siamese_cat', 0.08397927135229111), ('african_buffalo', 0.08353123813867569), ('ants', 0.06892219930887222), ('walrus', 0.05629545822739601), ('rat', 0.055904362350702286)]\n"
     ]
    }
   ],
   "source": [
    "# Find similar words to 'machine'\n",
    "similar_words = model.wv.most_similar('cat', topn=10)\n",
    "\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af887b8",
   "metadata": {},
   "source": [
    "### 4. Computing Similarity Scores Between Words\n",
    "\n",
    "You can compute similarity scores between two words using the `similarity` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6dd1a2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2737165\n",
      "0.08397927  *\n",
      "0.1394797  *\n",
      "0.012166994\n",
      "0.055904355\n",
      "0.007804998\n"
     ]
    }
   ],
   "source": [
    "# Compute similarity score between 'cat' and 'dog'\n",
    "similarity_score = model.wv.similarity('cat', 'bobcat')\n",
    "print(similarity_score)\n",
    "\n",
    "similarity_score = model.wv.similarity('cat', 'siamese_cat')\n",
    "print(similarity_score, ' *')\n",
    "\n",
    "similarity_score = model.wv.similarity('cat', 'siamese cat')\n",
    "print(similarity_score, ' *')\n",
    "\n",
    "similarity_score = model.wv.similarity('cat', 'dog')\n",
    "print(similarity_score)\n",
    "\n",
    "similarity_score = model.wv.similarity('cat', 'rat')\n",
    "print(similarity_score)\n",
    "\n",
    "similarity_score = model.wv.similarity('cat', 'ferret')\n",
    "print(similarity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db912ee6",
   "metadata": {},
   "source": [
    "# Gensim Word2Vec\n",
    "\n",
    "### Using Gensim Word2Vec\n",
    "\n",
    "Here are the key steps to use Gensim Word2Vec:\n",
    "\n",
    "1. Training a Word2Vec Model on a Custom Corpus\n",
    "2. Finding Similar Words\n",
    "3. Computing Similarity Scores Between Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6523c3c",
   "metadata": {},
   "source": [
    "### 1. Training a Word2Vec Model on a Custom Corpus\n",
    "\n",
    "You can train a Word2Vec model on your custom corpus. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5af52a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences : [['african_buffalo'], ['alligator'], ['amphibian'], ['amur_leopard'], ['ants'], ['bear'], ['bird'], ['blue_whale'], ['bobcat'], ['cat'], ['chimp'], ['chimpanzee'], ['cow'], ['dog'], ['dolphin'], [], ['eagle'], ['elephant'], ['fish'], ['frog'], ['giant'], ['giant_panda'], ['goat'], ['gorilla'], ['hen'], ['horse'], ['killer_whale'], ['lion'], ['lizard'], ['monkey'], ['mouse'], ['orangutan'], ['ostrich'], ['ox'], ['panda'], ['polar_bear'], ['rabbit'], ['rat'], ['rhino'], ['rhinoceros'], ['rhinoceroses'], ['seal'], ['sealskin'], ['siamese_cat'], ['skunk'], ['spider_monkey'], ['squirrel'], ['tiger'], ['turtle'], ['walrus'], ['whale'], ['bird'], ['fish'], ['lion'], ['tiger'], ['bull']] \n",
      "\n",
      "model : Word2Vec<vocab=51, vector_size=300, alpha=0.025> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\n",
    "    'african_buffalo', 'alligator', 'amphibian', 'amur_leopard', \n",
    "    'ants', 'bear', 'bird', 'blue_whale', 'bobcat', 'cat', 'chimp', \n",
    "    'chimpanzee', 'cow', 'dog', 'dolphin', 'domestic_water_buffalo', \n",
    "    'eagle', 'elephant', 'fish', 'frog', 'giant', 'giant_panda', 'goat', \n",
    "    'gorilla', 'hen', 'horse', 'killer_whale', 'lion', 'lizard', 'monkey', \n",
    "    'mouse', 'orangutan', 'ostrich', 'ox', 'panda', 'polar_bear', 'rabbit', \n",
    "    'rat', 'rhino', 'rhinoceros', 'rhinoceroses', 'seal', 'sealskin', \n",
    "    'siamese_cat', 'skunk', 'spider_monkey', 'squirrel', 'tiger', 'turtle', \n",
    "    'walrus', 'whale', 'bird', 'fish', 'lion', 'tiger', 'bull'\n",
    "]\n",
    "\n",
    "# Preprocess sentences\n",
    "sentences = [simple_preprocess(sentence) for sentence in sentences]\n",
    "print(f\"sentences : {sentences} \\n\")\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=300, window=5, min_count=1, epochs=10000)\n",
    "print(f\"model : {model} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7817e4",
   "metadata": {},
   "source": [
    "### 2. Finding Similar Words\n",
    "\n",
    "Once you have a trained model, you can find similar words using the most_similar method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62fc0751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to 'cat':\n",
      "ants: 0.11463356018066406\n",
      "goat: 0.10705526173114777\n",
      "rhinoceros: 0.09309180825948715\n",
      "monkey: 0.09122835099697113\n",
      "rhino: 0.08179710805416107\n",
      "rabbit: 0.07725443691015244\n",
      "orangutan: 0.07632383704185486\n",
      "bear: 0.07548326253890991\n",
      "rhinoceroses: 0.06613056361675262\n",
      "chimpanzee: 0.042745448648929596\n"
     ]
    }
   ],
   "source": [
    "# Find similar words to 'cat'\n",
    "similar_words = model.wv.most_similar('cat', topn=10)\n",
    "print(\"Most similar words to 'cat':\")\n",
    "for word, score in similar_words:\n",
    "    print(f\"{word}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e920c777",
   "metadata": {},
   "source": [
    "### 3. Computing Similarity Scores Between Words\n",
    "\n",
    "You can compute similarity scores between two words using the similarity method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3dbd170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score between 'cat' and 'bobcat': -0.01531929150223732\n"
     ]
    }
   ],
   "source": [
    "# Compute similarity score between 'cat' and 'bobcat'\n",
    "similarity_score = model.wv.similarity('cat', 'bobcat')\n",
    "print(f\"Similarity score between 'cat' and 'bobcat': {similarity_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516ad688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e320a232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04fe8e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7417b958",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8933cd1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df0d763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99701e40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aaedca56",
   "metadata": {},
   "source": [
    "# ZSL\n",
    "\n",
    "Zero-shot learning (ZSL) is a machine learning paradigm where a model is trained to recognize objects or perform tasks that it has never seen before during training. Instead of relying solely on labeled examples for every possible category, ZSL leverages auxiliary information (such as semantic attributes, descriptions, or relationships) to make predictions about unseen classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914570e9",
   "metadata": {},
   "source": [
    "# Code for Zero-Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "444d73a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProjectWork\\Basic_Python\\Zero_shot_Learning\\venv_zsl\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fasttext'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m normalize\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cdist\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfasttext\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load a pre-trained ResNet50 model\u001b[39;00m\n\u001b[0;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m ResNet50(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m'\u001b[39m, include_top\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, pooling\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fasttext'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.spatial.distance import cdist\n",
    "import fasttext\n",
    "\n",
    "# Load a pre-trained ResNet50 model\n",
    "model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "# Function to extract visual features\n",
    "def extract_features(image_path):\n",
    "    image = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))\n",
    "    image = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image = tf.keras.applications.resnet50.preprocess_input(image)\n",
    "    features = model.predict(image)\n",
    "    return features\n",
    "\n",
    "# Load FastText word vectors\n",
    "fasttext_model = fasttext.load_model('cc.en.300.bin')\n",
    "# from gensim.models.keyedvectors import KeyedVectors\n",
    "# import gensim.downloader as api\n",
    "# fast_text_vectors = api.load(\"fasttext-wiki-news-subwords-300\")\n",
    "# fast_text_vectors.save('fstwk_1.d2v')\n",
    "# fast_text_vectors = KeyedVectors.load(\"fstwk_1.d2v\")\n",
    "\n",
    "# Example seen and unseen classes\n",
    "seen_classes = ['cat', 'dog', 'horse']\n",
    "unseen_classes = ['lion', 'tiger', 'elephant']\n",
    "\n",
    "# Get word vectors for classes\n",
    "def get_class_vectors(classes):\n",
    "    return np.array([fasttext_model.get_word_vector(cls) for cls in classes])\n",
    "\n",
    "# Normalize the word vectors\n",
    "seen_vectors = normalize(get_class_vectors(seen_classes))\n",
    "unseen_vectors = normalize(get_class_vectors(unseen_classes))\n",
    "\n",
    "# Function to perform zero-shot classification\n",
    "def zero_shot_classify(image_path):\n",
    "    features = extract_features(image_path)\n",
    "    features = normalize(features)\n",
    "    distances = cdist(features, unseen_vectors, metric='cosine')\n",
    "    return unseen_classes[np.argmin(distances)]\n",
    "\n",
    "# Example usage\n",
    "image_path = 'Sample_Images\\cat1.jpg'\n",
    "predicted_class = zero_shot_classify(image_path)\n",
    "print(f'Predicted class: {predicted_class}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7076fe16",
   "metadata": {},
   "source": [
    "Conditional Autoencoders (CAEs) are a variant of autoencoders where additional information is used to condition the encoding and decoding processes. This conditioning can help the autoencoder learn more structured and relevant representations based on the context provided by the additional information.\n",
    "\n",
    "### Autoencoders Recap\n",
    "\n",
    "Autoencoders are neural networks designed to learn efficient representations (encodings) of input data, typically for the purpose of dimensionality reduction or data denoising.\n",
    "\n",
    "Components:\n",
    "- Encoder: Compresses the input data into a latent-space representation.\n",
    "- Decoder: Reconstructs the input data from the latent representation.\n",
    "\n",
    "### Conditional Autoencoders\n",
    "In a Conditional Autoencoder, the input data is conditioned on some additional information (conditions). This information can be labels, attributes, or any other relevant context that influences the encoding and decoding processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a7ac74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377655d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead62f35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d30cd91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bf8bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
